<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>secfsdstools.a_utils.parallelexecution API documentation</title>
<meta name="description" content="Helper Utils to execute tasks in parallel â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>secfsdstools.a_utils.parallelexecution</code></h1>
</header>
<section id="section-intro">
<p>Helper Utils to execute tasks in parallel.</p>
<p>using pathos.multiprocessing instead of multiprocessing, so that method functions can be used
-&gt; pypi.org "pathos"</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Helper Utils to execute tasks in parallel.

using pathos.multiprocessing instead of multiprocessing, so that method functions can be used
-&gt; pypi.org &#34;pathos&#34;
&#34;&#34;&#34;

import logging
from time import time, sleep
from typing import Generic, TypeVar, List, Callable, Optional, Tuple

from pathos.multiprocessing import ProcessingPool as Pool
from pathos.multiprocessing import cpu_count

IT = TypeVar(&#34;IT&#34;)  # input type of the list to split
PT = TypeVar(&#34;PT&#34;)  # processed type of the list to split
OT = TypeVar(&#34;OT&#34;)  # PostProcessed Type


class ParallelExecutor(Generic[IT, PT, OT]):
    &#34;&#34;&#34;
    this helper class supports in parallel processing of entries that are provided
    as a list, for instance to     download and process data.. like downloading and
    processing sec filing reports.
    It mainly takes care about the following things:
    - it throttles the amount of calls
      for instance, you can define how many call should be made at max within a second.
      When you download content from the SEC, you may only send 10 request per second.
    - retry handling
      especially when reading data from the web, it is normal that some requests might
      end up with an exception. If that happens, the logic retries failed entries.
    - processing in chunks
      you can define the chunksize after which the state is updated. For instance
      when having to download and process a few thousand reports, you can make sure
      that the current state is stored every 100 reports. Therefore in case
      something &#34;bigger&#34; happens, at least the work that already was done is kept.


    How to use it
    You need to define 3 functions.
    The get_entries_function returnes a list with the entries that need to be processed.
    The process_element_function is the logic that processes a single element from the
    entries and returns the processed content.
    The post_process_chunk_function receives a list of processed entries. you use this
    function to update the processed entries, so that in the next call to
    get_entries_function, these entries will not be part of
    &#34;&#34;&#34;

    def __init__(self,
                 processes: int = cpu_count(),
                 chunksize: int = 100,
                 max_calls_per_sec: int = 0,
                 intend: str = &#34;    &#34;,
                 execute_serial: bool = False):
        &#34;&#34;&#34;
        Args:
            processes (int, optional, cpu_count()): number of parallel processes,
             default is cpu_count
            chunksize (int, optional, 100): size of chunk - think of it as a commit,
             default is 100
            max_calls_per_sec (int, optional, 0): how many calls may be made per
             second (for all processes), default is 0, meaning no limit
            intend (str, optional, &#39;    &#39;): how much log messages should be intended
            execute_serial (bool, optional, False): for easier debugging, this
             flag ensures that all data areprocessed in the main thread
        &#34;&#34;&#34;

        self.processes = processes
        self.chunksize = chunksize
        self.intend = intend
        self.execute_serial = execute_serial
        self.min_roundtrip_time = 0
        self.max_calls_per_sec = max_calls_per_sec
        if max_calls_per_sec &gt; 0:
            if execute_serial:
                self.min_roundtrip_time = 1 / max_calls_per_sec
            else:
                self.min_roundtrip_time = float(processes) / max_calls_per_sec

        self.pool = Pool(self.processes)

        self.get_entries_function: Optional[Callable[[], List[IT]]] = None
        self.process_element_function: Optional[Callable[[IT], PT]] = None
        self.post_process_chunk_function: Optional[Callable[[List[PT]], List[OT]]] = None

    def set_get_entries_function(self, get_entries: Callable[[], List[IT]]):
        &#34;&#34;&#34;
        set the function which returns the list of the items that have not been processed.

        Args:
            get_entries (Callable[[], List[IT]]): function that returns the items to be processed

        &#34;&#34;&#34;
        self.get_entries_function = get_entries

    def set_process_element_function(self, process_element: Callable[[IT], PT]):
        &#34;&#34;&#34;
        set the function that processes a single element and returns the processed element

        Args:
            process_element (Callable[[IT], PT]): function that processes a single element
        &#34;&#34;&#34;
        self.process_element_function = process_element

    def set_post_process_chunk_function(self, post_process: Callable[[List[PT]], List[OT]]):
        &#34;&#34;&#34;
        set the function that receives a list of processed elements and updates
        the state of these elements accordingly

        Args:
            post_process (Callable[[List[PT]], List[OT]]): the post process method
        &#34;&#34;&#34;
        self.post_process_chunk_function = post_process

    def _process_throttled(self, data: IT) -&gt; PT:
        &#34;&#34;&#34;
        process the current data set and makes sure that only a limited number
        of calls per seconds are made
        &#34;&#34;&#34;
        start = time()
        result: PT = self.process_element_function(data)
        end = time()
        if self.min_roundtrip_time &gt; 0:
            sleep_time = max(0.0, self.min_roundtrip_time - (end - start))
            sleep(sleep_time)

        return result

    def _execute_parallel(self, chunk: List[IT]) -&gt; List[PT]:
        return self.pool.map(self._process_throttled, chunk)

    def _execute_serial(self, chunk: List[IT]) -&gt; List[PT]:
        results: List[PT] = []
        for entry in chunk:
            results.append(self._process_throttled(entry))
        return results

    def execute(self) -&gt; Tuple[List[OT], List[IT]]:
        &#34;&#34;&#34;
        starts the parallel processing and returns the results.

        Returns:
             Tuple[List[OT], List[IT]]: tuple with two lists: the first are the processed entries,
                 the second list are the entries that couldn&#39;t be processed
        &#34;&#34;&#34;

        last_missing = None
        missing: List[IT] = self.get_entries_function()
        result_list: List[OT] = []

        # we retry as long as we were able to process additional entries with in the while loop.
        while (last_missing is None) or (last_missing &gt; len(missing)):
            last_missing = len(missing)
            logging.info(&#34;%smissing entries %d&#34;, self.intend, len(missing))

            # break up the list of missing entries in chunks and process every chunk in parallel
            chunk_entries = self.chunksize
            # if chunksize is zero, we just create a single chunk
            if self.chunksize == 0:
                chunk_entries = len(missing)

            for i in range(0, len(missing), chunk_entries):
                chunk = missing[i:i + chunk_entries]

                processed: List[PT]

                if self.execute_serial:
                    processed = self._execute_serial(chunk)
                else:
                    processed = self._execute_parallel(chunk)

                # post process the chunk and add the result to the result_list.
                # it is olso ok to return nothing
                result_list.extend(self.post_process_chunk_function(processed))
                logging.info(&#34;%scommited chunk: %d&#34;, self.intend, i)

            # call get_entries_function again to check whether there have been
            # entries that couldn&#39;t be processed
            missing = self.get_entries_function()

        return result_list, missing</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="secfsdstools.a_utils.parallelexecution.ParallelExecutor"><code class="flex name class">
<span>class <span class="ident">ParallelExecutor</span></span>
<span>(</span><span>processes:Â intÂ =Â 2, chunksize:Â intÂ =Â 100, max_calls_per_sec:Â intÂ =Â 0, intend:Â strÂ =Â '
', execute_serial:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>this helper class supports in parallel processing of entries that are provided
as a list, for instance to
download and process data.. like downloading and
processing sec filing reports.
It mainly takes care about the following things:
- it throttles the amount of calls
for instance, you can define how many call should be made at max within a second.
When you download content from the SEC, you may only send 10 request per second.
- retry handling
especially when reading data from the web, it is normal that some requests might
end up with an exception. If that happens, the logic retries failed entries.
- processing in chunks
you can define the chunksize after which the state is updated. For instance
when having to download and process a few thousand reports, you can make sure
that the current state is stored every 100 reports. Therefore in case
something "bigger" happens, at least the work that already was done is kept.</p>
<p>How to use it
You need to define 3 functions.
The get_entries_function returnes a list with the entries that need to be processed.
The process_element_function is the logic that processes a single element from the
entries and returns the processed content.
The post_process_chunk_function receives a list of processed entries. you use this
function to update the processed entries, so that in the next call to
get_entries_function, these entries will not be part of</p>
<h2 id="args">Args</h2>
<dl>
<dt>processes (int, optional, cpu_count()): number of parallel processes,</dt>
<dt>default is cpu_count</dt>
<dt><strong><code>chunksize</code></strong> :&ensp;<code>int</code>, optional<code>, 100</code></dt>
<dd>size of chunk - think of it as a commit,</dd>
<dt>default is 100</dt>
<dt><strong><code>max_calls_per_sec</code></strong> :&ensp;<code>int</code>, optional<code>, 0</code></dt>
<dd>how many calls may be made per</dd>
<dt>second (for all processes), default is 0, meaning no limit</dt>
<dt>intend (str, optional, '
'): how much log messages should be intended</dt>
<dt><strong><code>execute_serial</code></strong> :&ensp;<code>bool</code>, optional<code>, False</code></dt>
<dd>for easier debugging, this</dd>
</dl>
<p>flag ensures that all data areprocessed in the main thread</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ParallelExecutor(Generic[IT, PT, OT]):
    &#34;&#34;&#34;
    this helper class supports in parallel processing of entries that are provided
    as a list, for instance to     download and process data.. like downloading and
    processing sec filing reports.
    It mainly takes care about the following things:
    - it throttles the amount of calls
      for instance, you can define how many call should be made at max within a second.
      When you download content from the SEC, you may only send 10 request per second.
    - retry handling
      especially when reading data from the web, it is normal that some requests might
      end up with an exception. If that happens, the logic retries failed entries.
    - processing in chunks
      you can define the chunksize after which the state is updated. For instance
      when having to download and process a few thousand reports, you can make sure
      that the current state is stored every 100 reports. Therefore in case
      something &#34;bigger&#34; happens, at least the work that already was done is kept.


    How to use it
    You need to define 3 functions.
    The get_entries_function returnes a list with the entries that need to be processed.
    The process_element_function is the logic that processes a single element from the
    entries and returns the processed content.
    The post_process_chunk_function receives a list of processed entries. you use this
    function to update the processed entries, so that in the next call to
    get_entries_function, these entries will not be part of
    &#34;&#34;&#34;

    def __init__(self,
                 processes: int = cpu_count(),
                 chunksize: int = 100,
                 max_calls_per_sec: int = 0,
                 intend: str = &#34;    &#34;,
                 execute_serial: bool = False):
        &#34;&#34;&#34;
        Args:
            processes (int, optional, cpu_count()): number of parallel processes,
             default is cpu_count
            chunksize (int, optional, 100): size of chunk - think of it as a commit,
             default is 100
            max_calls_per_sec (int, optional, 0): how many calls may be made per
             second (for all processes), default is 0, meaning no limit
            intend (str, optional, &#39;    &#39;): how much log messages should be intended
            execute_serial (bool, optional, False): for easier debugging, this
             flag ensures that all data areprocessed in the main thread
        &#34;&#34;&#34;

        self.processes = processes
        self.chunksize = chunksize
        self.intend = intend
        self.execute_serial = execute_serial
        self.min_roundtrip_time = 0
        self.max_calls_per_sec = max_calls_per_sec
        if max_calls_per_sec &gt; 0:
            if execute_serial:
                self.min_roundtrip_time = 1 / max_calls_per_sec
            else:
                self.min_roundtrip_time = float(processes) / max_calls_per_sec

        self.pool = Pool(self.processes)

        self.get_entries_function: Optional[Callable[[], List[IT]]] = None
        self.process_element_function: Optional[Callable[[IT], PT]] = None
        self.post_process_chunk_function: Optional[Callable[[List[PT]], List[OT]]] = None

    def set_get_entries_function(self, get_entries: Callable[[], List[IT]]):
        &#34;&#34;&#34;
        set the function which returns the list of the items that have not been processed.

        Args:
            get_entries (Callable[[], List[IT]]): function that returns the items to be processed

        &#34;&#34;&#34;
        self.get_entries_function = get_entries

    def set_process_element_function(self, process_element: Callable[[IT], PT]):
        &#34;&#34;&#34;
        set the function that processes a single element and returns the processed element

        Args:
            process_element (Callable[[IT], PT]): function that processes a single element
        &#34;&#34;&#34;
        self.process_element_function = process_element

    def set_post_process_chunk_function(self, post_process: Callable[[List[PT]], List[OT]]):
        &#34;&#34;&#34;
        set the function that receives a list of processed elements and updates
        the state of these elements accordingly

        Args:
            post_process (Callable[[List[PT]], List[OT]]): the post process method
        &#34;&#34;&#34;
        self.post_process_chunk_function = post_process

    def _process_throttled(self, data: IT) -&gt; PT:
        &#34;&#34;&#34;
        process the current data set and makes sure that only a limited number
        of calls per seconds are made
        &#34;&#34;&#34;
        start = time()
        result: PT = self.process_element_function(data)
        end = time()
        if self.min_roundtrip_time &gt; 0:
            sleep_time = max(0.0, self.min_roundtrip_time - (end - start))
            sleep(sleep_time)

        return result

    def _execute_parallel(self, chunk: List[IT]) -&gt; List[PT]:
        return self.pool.map(self._process_throttled, chunk)

    def _execute_serial(self, chunk: List[IT]) -&gt; List[PT]:
        results: List[PT] = []
        for entry in chunk:
            results.append(self._process_throttled(entry))
        return results

    def execute(self) -&gt; Tuple[List[OT], List[IT]]:
        &#34;&#34;&#34;
        starts the parallel processing and returns the results.

        Returns:
             Tuple[List[OT], List[IT]]: tuple with two lists: the first are the processed entries,
                 the second list are the entries that couldn&#39;t be processed
        &#34;&#34;&#34;

        last_missing = None
        missing: List[IT] = self.get_entries_function()
        result_list: List[OT] = []

        # we retry as long as we were able to process additional entries with in the while loop.
        while (last_missing is None) or (last_missing &gt; len(missing)):
            last_missing = len(missing)
            logging.info(&#34;%smissing entries %d&#34;, self.intend, len(missing))

            # break up the list of missing entries in chunks and process every chunk in parallel
            chunk_entries = self.chunksize
            # if chunksize is zero, we just create a single chunk
            if self.chunksize == 0:
                chunk_entries = len(missing)

            for i in range(0, len(missing), chunk_entries):
                chunk = missing[i:i + chunk_entries]

                processed: List[PT]

                if self.execute_serial:
                    processed = self._execute_serial(chunk)
                else:
                    processed = self._execute_parallel(chunk)

                # post process the chunk and add the result to the result_list.
                # it is olso ok to return nothing
                result_list.extend(self.post_process_chunk_function(processed))
                logging.info(&#34;%scommited chunk: %d&#34;, self.intend, i)

            # call get_entries_function again to check whether there have been
            # entries that couldn&#39;t be processed
            missing = self.get_entries_function()

        return result_list, missing</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="secfsdstools.a_utils.parallelexecution.ParallelExecutor.execute"><code class="name flex">
<span>def <span class="ident">execute</span></span>(<span>self) â€‘>Â Tuple[List[~OT],Â List[~IT]]</span>
</code></dt>
<dd>
<div class="desc"><p>starts the parallel processing and returns the results.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[List[OT], List[IT]]</code></dt>
<dd>tuple with two lists: the first are the processed entries,
the second list are the entries that couldn't be processed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute(self) -&gt; Tuple[List[OT], List[IT]]:
    &#34;&#34;&#34;
    starts the parallel processing and returns the results.

    Returns:
         Tuple[List[OT], List[IT]]: tuple with two lists: the first are the processed entries,
             the second list are the entries that couldn&#39;t be processed
    &#34;&#34;&#34;

    last_missing = None
    missing: List[IT] = self.get_entries_function()
    result_list: List[OT] = []

    # we retry as long as we were able to process additional entries with in the while loop.
    while (last_missing is None) or (last_missing &gt; len(missing)):
        last_missing = len(missing)
        logging.info(&#34;%smissing entries %d&#34;, self.intend, len(missing))

        # break up the list of missing entries in chunks and process every chunk in parallel
        chunk_entries = self.chunksize
        # if chunksize is zero, we just create a single chunk
        if self.chunksize == 0:
            chunk_entries = len(missing)

        for i in range(0, len(missing), chunk_entries):
            chunk = missing[i:i + chunk_entries]

            processed: List[PT]

            if self.execute_serial:
                processed = self._execute_serial(chunk)
            else:
                processed = self._execute_parallel(chunk)

            # post process the chunk and add the result to the result_list.
            # it is olso ok to return nothing
            result_list.extend(self.post_process_chunk_function(processed))
            logging.info(&#34;%scommited chunk: %d&#34;, self.intend, i)

        # call get_entries_function again to check whether there have been
        # entries that couldn&#39;t be processed
        missing = self.get_entries_function()

    return result_list, missing</code></pre>
</details>
</dd>
<dt id="secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_get_entries_function"><code class="name flex">
<span>def <span class="ident">set_get_entries_function</span></span>(<span>self, get_entries:Â Callable[[],Â List[~IT]])</span>
</code></dt>
<dd>
<div class="desc"><p>set the function which returns the list of the items that have not been processed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>get_entries</code></strong> :&ensp;<code>Callable[[], List[IT]]</code></dt>
<dd>function that returns the items to be processed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_get_entries_function(self, get_entries: Callable[[], List[IT]]):
    &#34;&#34;&#34;
    set the function which returns the list of the items that have not been processed.

    Args:
        get_entries (Callable[[], List[IT]]): function that returns the items to be processed

    &#34;&#34;&#34;
    self.get_entries_function = get_entries</code></pre>
</details>
</dd>
<dt id="secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_post_process_chunk_function"><code class="name flex">
<span>def <span class="ident">set_post_process_chunk_function</span></span>(<span>self, post_process:Â Callable[[List[~PT]],Â List[~OT]])</span>
</code></dt>
<dd>
<div class="desc"><p>set the function that receives a list of processed elements and updates
the state of these elements accordingly</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>post_process</code></strong> :&ensp;<code>Callable[[List[PT]], List[OT]]</code></dt>
<dd>the post process method</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_post_process_chunk_function(self, post_process: Callable[[List[PT]], List[OT]]):
    &#34;&#34;&#34;
    set the function that receives a list of processed elements and updates
    the state of these elements accordingly

    Args:
        post_process (Callable[[List[PT]], List[OT]]): the post process method
    &#34;&#34;&#34;
    self.post_process_chunk_function = post_process</code></pre>
</details>
</dd>
<dt id="secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_process_element_function"><code class="name flex">
<span>def <span class="ident">set_process_element_function</span></span>(<span>self, process_element:Â Callable[[~IT],Â ~PT])</span>
</code></dt>
<dd>
<div class="desc"><p>set the function that processes a single element and returns the processed element</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>process_element</code></strong> :&ensp;<code>Callable[[IT], PT]</code></dt>
<dd>function that processes a single element</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_process_element_function(self, process_element: Callable[[IT], PT]):
    &#34;&#34;&#34;
    set the function that processes a single element and returns the processed element

    Args:
        process_element (Callable[[IT], PT]): function that processes a single element
    &#34;&#34;&#34;
    self.process_element_function = process_element</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="secfsdstools.a_utils" href="index.html">secfsdstools.a_utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="secfsdstools.a_utils.parallelexecution.ParallelExecutor" href="#secfsdstools.a_utils.parallelexecution.ParallelExecutor">ParallelExecutor</a></code></h4>
<ul class="">
<li><code><a title="secfsdstools.a_utils.parallelexecution.ParallelExecutor.execute" href="#secfsdstools.a_utils.parallelexecution.ParallelExecutor.execute">execute</a></code></li>
<li><code><a title="secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_get_entries_function" href="#secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_get_entries_function">set_get_entries_function</a></code></li>
<li><code><a title="secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_post_process_chunk_function" href="#secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_post_process_chunk_function">set_post_process_chunk_function</a></code></li>
<li><code><a title="secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_process_element_function" href="#secfsdstools.a_utils.parallelexecution.ParallelExecutor.set_process_element_function">set_process_element_function</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>