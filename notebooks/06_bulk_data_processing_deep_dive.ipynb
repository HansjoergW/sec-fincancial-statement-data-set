{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5974588b-0809-468d-b6e3-e2df73afaacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ensure that all columns are shown and that colum content is not cut\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.max_rows', 500) # ensure that all rows are shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f3746-2750-4526-a4d9-b97ca6a1e167",
   "metadata": {},
   "source": [
    "# Bulk Data Processing Deep Dive\n",
    "The main advantage of this library is that the all data is downloaded down to your computer and therefore makes it easy to analyze all the data at once. \n",
    "\n",
    "For instance, if you want to implement your own screener.\n",
    "\n",
    "Just on the file system, the size of all data files is more than 2 GB. Since the parquet format is also storage optimized, loading all the data into memory would need significantly more memory than a standard computer/laptop usually provides.\n",
    "\n",
    "Hence it is important to filter the data during the loading process, so that you only load the data into memory that is really needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba379f1-5124-4bc4-bd2d-3cfc292ba03f",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "In the first part of this notebook, we will create different datasets for all the balance sheet datapoints, the cashflow datapoints, and the income statement datapoints.\n",
    "These datasets will be stored in their own directories, so that they can be easily loaded afterwards. Moreover, we will store the raw version (where the num_df and the pre_df are not joined) and the joined version, where num_df and pre_df are joined. Depending on what you want to do/analyze, you can use either one.\n",
    "\n",
    "**Note:** The code that is explained here is also available in the modul `bulk_loading` which is inside the `u_usecase` package.\n",
    "\n",
    "This notebook will show two approaches. The first one is loading all the data in parallel, which you can do if you have enough resources in your computer. The second is doing it sequentially, which is slower, but needs less memory. In the end, you will create these datasets once and extend it when new quarterly zip files arrive, or you will recreate them once every quarter. So in the end it doesn't really matter if the process takes 15 minutes or an hour.\n",
    "\n",
    "We will also apply different filters:\n",
    "\n",
    "* only filter 10-K and 10-Q reports during loading\n",
    "* `ReportPeriodRawFilter`: since we are only interested in datapoints that belong to the period of the report\n",
    "* `MainCoregRawFilter`: since we don't want to see datapoints of a subsidiary\n",
    "* `OfficialTagsOnlyRawFilter`: since we want to be able to compare the content and therefore don't want to read tags that or not in the standard sec xbrl definition\n",
    "* `USDOnlyRawFilter`: since we are not interested in money datapoints that are not in USD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1904d65-28df-47e1-9385-9b60098afc6e",
   "metadata": {},
   "source": [
    "### Basics\n",
    "First, we will defines some basic stuff that is used by both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d01c1f-8479-4360-aab6-164068424126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from secfsdstools.d_container.databagmodel import RawDataBag, JoinedDataBag\n",
    "from secfsdstools.e_collector.zipcollecting import ZipCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e87306-9242-4523-9b81-0293069ddebe",
   "metadata": {},
   "source": [
    "The following list defines which statements we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594116ef-3645-4dcc-9942-201bfe343e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statements_to_load = [\"BS\", \"CF\", \"IS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b535c-9ad4-4b00-9d39-bdf3dad43663",
   "metadata": {},
   "source": [
    "Next, we define a filter function, that defines the whole chain. As mentioned in the 04_collector_deep_dive.ipynt notebook, we have to define the imports inside the function itself, if we want to use it in jupyter together with parallization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d908348-8e48-461f-a26c-ec0d0f2bbfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postloadfilter(databag: RawDataBag) -> RawDataBag:\n",
    "    from secfsdstools.e_filter.rawfiltering import ReportPeriodRawFilter, MainCoregRawFilter, OfficialTagsOnlyRawFilter, USDOnlyRawFilter\n",
    "\n",
    "    return databag[ReportPeriodRawFilter()][MainCoregRawFilter()][OfficialTagsOnlyRawFilter()][USDOnlyRawFilter()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff096f-b7e8-4e4a-a661-2a95e4b922f5",
   "metadata": {},
   "source": [
    "Next is a simple function that takes a raw databag and creates the joined databag. Both, the rawdatabag and the joined databag are then stored in a specific folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9395fe8a-06c3-4ca4-9953-fc22fa58b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_databag(databag: RawDataBag, financial_statement: str, base_path: str) -> JoinedDataBag:\n",
    "    target_path_raw = os.path.join(base_path, financial_statement, 'raw')\n",
    "    print(f\"store rawdatabag under {target_path_raw}\")\n",
    "    os.makedirs(target_path_raw, exist_ok=True)\n",
    "    databag.save(target_path_raw)\n",
    "    \n",
    "    target_path_joined = os.path.join(base_path, financial_statement, 'joined')\n",
    "    os.makedirs(target_path_joined, exist_ok=True)\n",
    "    print(\"create joined databag\")\n",
    "    joined_databag = databag.join()\n",
    "    \n",
    "    print(f\"store joineddatabag under {target_path_joined}\")\n",
    "    joined_databag.save(target_path_joined)\n",
    "    return joined_databag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564ba5e-3334-47a9-94c7-53ba5df7a9c5",
   "metadata": {},
   "source": [
    "### Parallel Data Loading\n",
    "As stated above, we want to load all available 10-K and 10-Q reports. Therefore, we can use the `ZipCollector`which provides an option to load data from all available zip files. \n",
    "\n",
    "Moreover, the implementation of the ziploader uses all your cores in order to load data from your disk into memory. So you don't have to implement the parallization yourself. There are 50+ zip files that have to be loaded, so if you have 4 cores, you will load 4 at one time.\n",
    "\n",
    "Also, the `ZipCollector` provides parameters for filtering the report type (10-K and 10-Q) amd the financial statement type (Balance Sheet, Casch Flow, or Income Statement). These filters are directly applied during loading, since the data is stored in Parquet format. This will already reduce that amount of data that is being loaded into memory significantly.\n",
    "\n",
    "Moreover, it also provides the post_load_filter which we can use to apply the other filters, defined in the postloadfilter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18ac72e-0020-4ea5-9cd8-373e2f143fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_financial_statements_parallel(financial_statement: str) -> RawDataBag:\n",
    "    \"\"\" \n",
    "    financial_statement: either \"BS\", \"CF\", or \"IS\"\n",
    "    \"\"\"\n",
    "\n",
    "    collector: ZipCollector = ZipCollector.get_all_zips(forms_filter=[\"10-K\", \"10-Q\"],\n",
    "                                                        stmt_filter=[financial_statement],\n",
    "                                                        post_load_filter=postloadfilter)\n",
    "    return collector.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db367d8c-bda3-4ab1-a555-0e31c6f6bd52",
   "metadata": {},
   "source": [
    "We loop over the statements that we want to load and collect their datapoints into a specific dataset.\n",
    "\n",
    "This process will take several minutes. On my laptop the execution time was approximately 16 minutes (32GB Ram / 4/8 Cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b158ec71-ce9d-49ea-9a23-6f111fcc8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:30:59,104 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2023-12-05 06:30:59,170 [INFO] updateprocess  Check if new report zip files are available...\n",
      "2023-12-05 06:30:59,249 [INFO] updateprocess  check if there are new files to download from sec.gov ...\n",
      "2023-12-05 06:31:00,295 [INFO] updateprocess  start to transform to parquet format ...\n",
      "2023-12-05 06:31:00,311 [INFO] updateprocess  start to index parquet files ...\n",
      "2023-12-05 06:31:00,375 [INFO] parallelexecution      items to process: 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rapid-api-key is set: \n",
      "If you are interested in daily updates, please have a look at https://rapidapi.com/hansjoerg.wingeier/api/daily-sec-financial-statement-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:35:21,228 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/BS\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/BS\\joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:36:54,241 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2023-12-05 06:36:54,288 [INFO] parallelexecution      items to process: 58\n",
      "2023-12-05 06:41:06,636 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/CF\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/CF\\joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:42:38,230 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2023-12-05 06:42:38,269 [INFO] parallelexecution      items to process: 58\n",
      "2023-12-05 06:47:02,662 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/IS\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/IS\\joined\n"
     ]
    }
   ],
   "source": [
    "for statement_to_load in statements_to_load:\n",
    "    rawdatabag = load_all_financial_statements_parallel(financial_statement=statement_to_load)\n",
    "    save_databag(databag=rawdatabag, financial_statement=statement_to_load, base_path=\"./set/parallel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b0d7d-683d-40fd-937c-7242c5eb84ce",
   "metadata": {},
   "source": [
    "After processing, you have the following structure and sizes (with data up to 2023 Q3):\n",
    "<pre>\n",
    "- set/parallel\n",
    "  - BS\n",
    "    - raw     : 715 MB\n",
    "    - joined  : 266 MB\n",
    "  - CF\n",
    "    - raw     : 700 MB\n",
    "    - joined  : 246 MB\n",
    "  - IS\n",
    "    - raw     : 636 MB\n",
    "    - joined  : 217 MB\n",
    "</pre>\n",
    "\n",
    "Especially the joined databags have a size that can be easily loaded. Moreover, loading them just takes a few seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686cf90c-fe3e-4f28-a987-42d5961695fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded BS databag:  (10430891, 16)\n",
      "loaded CF databag:  (9468009, 16)\n",
      "loaded IS databag:  (9512425, 16)\n"
     ]
    }
   ],
   "source": [
    "#load BS joined data\n",
    "joinedBS = JoinedDataBag.load(\"./set/parallel/BS/joined\")\n",
    "print(\"loaded BS databag: \", joinedBS.pre_num_df.shape)\n",
    "joinedCF = JoinedDataBag.load(\"./set/parallel/CF/joined\")\n",
    "print(\"loaded CF databag: \", joinedCF.pre_num_df.shape)\n",
    "joinedIS = JoinedDataBag.load(\"./set/parallel/IS/joined\")\n",
    "print(\"loaded IS databag: \", joinedIS.pre_num_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15bf60-8266-405b-bf17-f1470c48933b",
   "metadata": {},
   "source": [
    "### Serial Data Loading\n",
    "As mentioned in above, parallel loading requires some minimal ressources on your laptop/computer. However, using a serial process, you still can create the databags for all balance sheet, cash flow, and income statments. Of course, we need more code and we will also save intermediate results on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d7a4d-1e01-4349-b400-fcadfea40fb5",
   "metadata": {},
   "source": [
    "The first thing which we need, is a list of all available zip-files. Actually, we just can copy the code from `ZipCollector.get_all_zips()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96fdc620-20d7-4d3b-9294-9b1b696241e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from secfsdstools.a_config.configmgt import ConfigurationManager\n",
    "from secfsdstools.c_index.indexdataaccess import ParquetDBIndexingAccessor\n",
    "\n",
    "def read_all_zip_names() -> List[str]:\n",
    "    configuration = ConfigurationManager.read_config_file()\n",
    "    dbaccessor = ParquetDBIndexingAccessor(db_dir=configuration.db_dir)\n",
    "\n",
    "    # exclude 2009q1.zip, since this is empty and causes an error when it is read with a filter\n",
    "    filenames = [x.fileName for x in dbaccessor.read_all_indexfileprocessing() if not x.fullPath.endswith(\"2009q1.zip\")]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2f566bf-0422-40dc-8d2f-b2bce2a842c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 12:40:16,684 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "['2019q4.zip', '2023q1.zip', '2014q3.zip', '2016q3.zip', '2018q1.zip', '2013q4.zip', '2015q2.zip', '2021q3.zip', '2010q3.zip', '2011q4.zip', '2012q2.zip', '2010q4.zip', '2016q1.zip', '2021q1.zip', '2011q2.zip', '2009q2.zip', '2022q1.zip', '2012q4.zip', '2010q1.zip', '2015q1.zip', '2022q3.zip', '2018q2.zip', '2019q3.zip', '2020q2.zip', '2022q4.zip', '2017q2.zip', '2012q3.zip', '2011q1.zip', '2017q4.zip', '2010q2.zip', '2018q3.zip', '2021q4.zip', '2019q2.zip', '2013q1.zip', '2015q4.zip', '2009q3.zip', '2016q2.zip', '2013q3.zip', '2016q4.zip', '2017q3.zip', '2018q4.zip', '2023q2.zip', '2014q4.zip', '2011q3.zip', '2020q3.zip', '2014q2.zip', '2020q1.zip', '2012q1.zip', '2014q1.zip', '2019q1.zip', '2015q3.zip', '2017q1.zip', '2020q4.zip', '2013q2.zip', '2021q2.zip', '2022q2.zip', '2009q4.zip', '2023q3.zip']\n"
     ]
    }
   ],
   "source": [
    "all_zip_names = read_all_zip_names()\n",
    "print(len(all_zip_names))\n",
    "print(all_zip_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab160d-2b3c-439b-8fa1-ec87950138ce",
   "metadata": {},
   "source": [
    "**Prepare the temporary dataset**\n",
    "Next, prepare the data for every single zip-file. So for every zip-file, we collect the datapoints for BS, CF, and IS and apply the aove defined filters. The following functions takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d0d98e6-e6c5-418a-b775-320245dfecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tmp_set(financial_statement: str, file_names: List[str], target_path: str = \"set/tmp/\"):\n",
    "    \"\"\" This function reads the data in sequence from the provided list of zip file names. It filters according to the \n",
    "        defined financial_statement and stores the data in specific subfolders.\n",
    "        \n",
    "        the folder structure will look like\n",
    "        <target_path>/<file_name>/<financial_statement>/raw\n",
    "        <target_path>/<file_name>/<financial_statement>/joined                                       \n",
    "        \"\"\"\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        collector = ZipCollector.get_zip_by_name(name=file_name,\n",
    "                                 forms_filter=[\"10-K\", \"10-Q\"],\n",
    "                                 stmt_filter=[financial_statement],\n",
    "                                 post_load_filter=postloadfilter)\n",
    "\n",
    "        rawdatabag = collector.collect()\n",
    "\n",
    "        base_path = os.path.join(target_path, file_name)\n",
    "        # saving the raw databag, joining and saving the joined databag\n",
    "        save_databag(databag=rawdatabag, financial_statement=financial_statement, base_path=base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d62cba-893c-4f41-9b06-ae99f8b87244",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "We call the function for every statement (BS, CF, and IS).\n",
    "As a reference, running all three cells took about 12 minutes on my laptop (32GB Ram / 4/8 Cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae6366-922c-4f32-b63c-e0cc1eed53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_tmp_set(financial_statement=\"BS\", file_names=all_zip_names, target_path=\"set/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cfcac-5260-41de-a9dd-7ce7f890859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_tmp_set(financial_statement=\"CF\", file_names=all_zip_names, target_path=\"set/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24659372-b22b-42a2-b002-b28abdba7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_tmp_set(financial_statement=\"IS\", file_names=all_zip_names, target_path=\"set/tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e0305-9380-439d-8b60-bc26d22f4cd8",
   "metadata": {},
   "source": [
    "We know have subfolders for BS, CF, IS for every quarterly zipfile with the corresponding datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12720d14-6be6-4eb9-9b11-6c6c79df7773",
   "metadata": {},
   "source": [
    "**Create the rawdatabags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b3a9574-8895-4fe3-a9f0-1e48ef717bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def create_rawdatabag(financial_statement: str, target_path: str):\n",
    "    raw_files = glob(f\"./set/tmp/*/{financial_statement}/raw/\", recursive = True)    \n",
    "    raw_databags = [RawDataBag.load(file) for file in raw_files]\n",
    "    raw_databag = RawDataBag.concat(raw_databags)\n",
    "    target_path_raw = os.path.join(target_path, financial_statement, 'raw')\n",
    "    print(f\"store rawdatabag under {target_path_raw}\")\n",
    "    os.makedirs(target_path_raw, exist_ok=True)\n",
    "    raw_databag.save(target_path_raw)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6ce36-6b06-400b-9200-e45182d4b86c",
   "metadata": {},
   "source": [
    "Next, concatenate the raw datasets together. Again, as a reference it took about 5 minutes to create all three rawdatabags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097a4940-2f44-434c-be19-b48cae0d1016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under set/serial/BS\\raw\n"
     ]
    }
   ],
   "source": [
    "create_rawdatabag(financial_statement=\"BS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d036f268-d664-4e18-96cb-5f50deb50257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under set/serial/CF\\raw\n"
     ]
    }
   ],
   "source": [
    "create_rawdatabag(financial_statement=\"CF\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27464709-7766-4ca3-a95f-3d8fd668d866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under set/serial/IS\\raw\n"
     ]
    }
   ],
   "source": [
    "create_rawdatabag(financial_statement=\"IS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d429003-84a6-4e66-89d3-974d7629a442",
   "metadata": {},
   "source": [
    "**Create the joined databags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c513474-b29e-4034-a6a4-afefe0a2070c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def create_joineddatabag(financial_statement: str, target_path: str):\n",
    "    joined_files = glob(f\"./set/tmp/*/{financial_statement}/joined/\", recursive = True)\n",
    "    joined_databags = [JoinedDataBag.load(file) for file in joined_files]\n",
    "    joined_databag = JoinedDataBag.concat(joined_databags)\n",
    "    target_path_joined = os.path.join(target_path, financial_statement, 'joined')\n",
    "    print(f\"store joineddatabag under {target_path_joined}\")\n",
    "    os.makedirs(target_path_joined, exist_ok=True)\n",
    "    joined_databag.save(target_path_joined)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8b6e8-4b20-4462-a931-d64e09ac5d33",
   "metadata": {},
   "source": [
    "Finally, create the joined databags. To create all three datasets, it took about 90 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fb50096-a09b-4b26-b2cc-9a5374820d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store joineddatabag under set/serial/BS\\joined\n"
     ]
    }
   ],
   "source": [
    "create_joineddatabag(financial_statement=\"BS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8f9c719-17f6-4a85-aadb-4608bf685c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store joineddatabag under set/serial/CF\\joined\n"
     ]
    }
   ],
   "source": [
    "create_joineddatabag(financial_statement=\"CF\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "117528e8-c577-47b0-b5b2-876b44a2af6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store joineddatabag under set/serial/IS\\joined\n"
     ]
    }
   ],
   "source": [
    "create_joineddatabag(financial_statement=\"IS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f5417-9726-4584-84a5-b284274681cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can read back all three prepared joined datasets. This only takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b7f8a8d-3354-4e6b-8280-53c145338bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded BS databag:  (10430891, 16)\n",
      "loaded CF databag:  (9468009, 16)\n",
      "loaded IS databag:  (9512425, 16)\n"
     ]
    }
   ],
   "source": [
    "#load BS joined data\n",
    "joinedBS = JoinedDataBag.load(\"./set/serial/BS/joined\")\n",
    "print(\"loaded BS databag: \", joinedBS.pre_num_df.shape)\n",
    "joinedCF = JoinedDataBag.load(\"./set/serial/CF/joined\")\n",
    "print(\"loaded CF databag: \", joinedCF.pre_num_df.shape)\n",
    "joinedIS = JoinedDataBag.load(\"./set/serial/IS/joined\")\n",
    "print(\"loaded IS databag: \", joinedIS.pre_num_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d560e-0eac-40aa-abd8-3209951bdddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
