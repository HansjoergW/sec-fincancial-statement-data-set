{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5974588b-0809-468d-b6e3-e2df73afaacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ensure that all columns are shown and that colum content is not cut\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.max_rows', 500) # ensure that all rows are shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f3746-2750-4526-a4d9-b97ca6a1e167",
   "metadata": {},
   "source": [
    "# Bulk Data Processing\n",
    "The main advantage of this library is that the all data is downloaded down to your computer and therefore makes to analyze the whole datasets as a whole. For instance if you want to implement your own screener.\n",
    "\n",
    "Just on the file system, the size of all data files are more than 2 GB. Since the parquet format is also storage optimized, loading all the data into memory would need significantly more memory than a standard computer/laptop does provide.\n",
    "\n",
    "Hence it is important to filter the data during the loading process.\n",
    "\n",
    "So in this notebook, we will create different datasets for the balance sheet datapoints, the cashflow datapoints, and the income statement datapoints.\n",
    "These datasets will be stored in their own directories, so that they can be easily loaded afterwards. Moreover, we will store the raw version (where the num_df and the pre_df are not joined) and the joined version, where num_df and pre_df are joined.\n",
    "\n",
    "This notebook will show two approaches. The first one is loading all the data in parallel, which you can do if you have enough resources in your computer. The second is doing it sequentially, which is slower, but needs less memory.\n",
    "\n",
    "We will also apply different filters:\n",
    "\n",
    "* only filter 10-K and 10-Q reports during loading\n",
    "* `ReportPeriodRawFilter`: since we are only interested in datapoints that belong to the period of the report\n",
    "* `MainCoregRawFilter`: since we don't want to see datapoints of a subsidiary\n",
    "* `OfficialTagsOnlyRawFilter`: since we want to be able to compare the content and therefore don't want to read tags that or not in the standard sec xbrl definition\n",
    "* `USDOnlyRawFilter`: since we are not interested in money datapoints that are not in USD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1904d65-28df-47e1-9385-9b60098afc6e",
   "metadata": {},
   "source": [
    "## Basics\n",
    "First we will defines some basic stuff that is used by both approaches.\n",
    "We will start with the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d01c1f-8479-4360-aab6-164068424126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from secfsdstools.d_container.databagmodel import RawDataBag, JoinedDataBag\n",
    "from secfsdstools.e_collector.zipcollecting import ZipCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e87306-9242-4523-9b81-0293069ddebe",
   "metadata": {},
   "source": [
    "The following list defines which statements we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594116ef-3645-4dcc-9942-201bfe343e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statements_to_load = [\"BS\", \"CF\", \"IS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b535c-9ad4-4b00-9d39-bdf3dad43663",
   "metadata": {},
   "source": [
    "Next, we define a filter function, that defines the whole chain. As mentioned in the 04_collector_deep_dive.ipynt notebook, we have to define the imports inside the function itself, if we want to use it in jupyter together with parallization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d908348-8e48-461f-a26c-ec0d0f2bbfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postloadfilter(databag: RawDataBag) -> RawDataBag:\n",
    "    from secfsdstools.e_filter.rawfiltering import ReportPeriodRawFilter, MainCoregRawFilter, OfficialTagsOnlyRawFilter, USDOnlyRawFilter\n",
    "\n",
    "    return databag[ReportPeriodRawFilter()][MainCoregRawFilter()][OfficialTagsOnlyRawFilter()][USDOnlyRawFilter()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff096f-b7e8-4e4a-a661-2a95e4b922f5",
   "metadata": {},
   "source": [
    "Next is a simple function that takes a raw databag and creates the joined databag. Both, the rawdatabag and the joined databag are then stored in a specific folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9395fe8a-06c3-4ca4-9953-fc22fa58b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_databag(databag: RawDataBag, financial_statement: str, base_path: str) -> JoinedDataBag:\n",
    "    target_path_raw = os.path.join(base_path, financial_statement, 'raw')\n",
    "    print(f\"store rawdatabag under {target_path_raw}\")\n",
    "    os.makedirs(target_path_raw, exist_ok=True)\n",
    "    databag.save(target_path_raw)\n",
    "    \n",
    "    target_path_joined = os.path.join(base_path, financial_statement, 'joined')\n",
    "    os.makedirs(target_path_joined, exist_ok=True)\n",
    "    print(\"create joined databag\")\n",
    "    joined_databag = databag.join()\n",
    "    print(f\"store joineddatabag under {target_path_joined}\")\n",
    "    joined_databag.save(target_path_joined)\n",
    "    return joined_databag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564ba5e-3334-47a9-94c7-53ba5df7a9c5",
   "metadata": {},
   "source": [
    "## Parallel Data Loading\n",
    "As stated above, we want to load all available 10-K and 10-Q reports. Therefore, we can use the `ZipCollector`which provides an option to load data from all available zip files. \n",
    "\n",
    "Moreover, the implementation of the ziploader does using all your cores in order to load data from your disk into memory. So you don't have to implement the parallization yourself. There are 50+ zip files that have to be loaded, so if you have 4 cores, you will load 4 at one time.\n",
    "\n",
    "Also, the `ZipCollector` provides parameters for filtering the report type (10-K and 10-Q) amd the financial statement type (Balance Sheet, Casch Flow, or Income Statement). This filters are directly applied during loading, since the data is stored in Parquet format. This will already reduce that amount of data that is being loaded into memory significantly.\n",
    "\n",
    "Moreover, it also provides the post_load_filter which we can use to apply the other filters, defined in the postloadfilter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d18ac72e-0020-4ea5-9cd8-373e2f143fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_financial_statements_parallel(financial_statement: str) -> RawDataBag:\n",
    "    \"\"\" \n",
    "    financial_statement: either \"BS\", \"CF\", or \"IS\"\n",
    "    \"\"\"\n",
    "\n",
    "    collector: ZipCollector = ZipCollector.get_all_zips(forms_filter=[\"10-K\", \"10-Q\"],\n",
    "                                                        stmt_filter=[financial_statement],\n",
    "                                                        post_load_filter=postloadfilter)\n",
    "    return collector.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db367d8c-bda3-4ab1-a555-0e31c6f6bd52",
   "metadata": {},
   "source": [
    "Wrapped it together in simple loop where we iterate over the statements that we want to load. Depending on your logging configuration, you can see in the output of the terminal where you started jupyter which files are being processed.\n",
    "\n",
    "This process will take several minutes. On my laptop the execution time was approximately 16 mintues (32GB Ram / 4 Cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b158ec71-ce9d-49ea-9a23-6f111fcc8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:30:59,104 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2023-12-05 06:30:59,170 [INFO] updateprocess  Check if new report zip files are available...\n",
      "2023-12-05 06:30:59,249 [INFO] updateprocess  check if there are new files to download from sec.gov ...\n",
      "2023-12-05 06:31:00,295 [INFO] updateprocess  start to transform to parquet format ...\n",
      "2023-12-05 06:31:00,311 [INFO] updateprocess  start to index parquet files ...\n",
      "2023-12-05 06:31:00,375 [INFO] parallelexecution      items to process: 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rapid-api-key is set: \n",
      "If you are interested in daily updates, please have a look at https://rapidapi.com/hansjoerg.wingeier/api/daily-sec-financial-statement-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:35:21,228 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/BS\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/BS\\joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:36:54,241 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2023-12-05 06:36:54,288 [INFO] parallelexecution      items to process: 58\n",
      "2023-12-05 06:41:06,636 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/CF\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/CF\\joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:42:38,230 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2023-12-05 06:42:38,269 [INFO] parallelexecution      items to process: 58\n",
      "2023-12-05 06:47:02,662 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/IS\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/IS\\joined\n"
     ]
    }
   ],
   "source": [
    "for statement_to_load in statements_to_load:\n",
    "    rawdatabag = load_all_financial_statements_parallel(financial_statement=statement_to_load)\n",
    "    save_databag(databag=rawdatabag, financial_statement=statement_to_load, base_path=\"./set/parallel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b0d7d-683d-40fd-937c-7242c5eb84ce",
   "metadata": {},
   "source": [
    "After processing, you have the following structure and sizes (with data up to 2023 Q3):\n",
    "<pre>\n",
    "- set/parallel\n",
    "  - BS\n",
    "    - raw     : 715 MB\n",
    "    - joined  : 266 MB\n",
    "  - CF\n",
    "    - raw     : 700 MB\n",
    "    - joined  : 246 MB\n",
    "  - IS\n",
    "    - raw     : 636 MB\n",
    "    - joined  : 217 MB\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54335992-07ea-4077-ac8e-081bd17281f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frage: sind die post filter wirklich angewendet worden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "686cf90c-fe3e-4f28-a987-42d5961695fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load BS joined data\n",
    "joinedBS = JoinedDataBag.load(\"./set/parallel/BS/joined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56220514-4cdf-41e3-b553-4d25f6d8aaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10-Q', '10-K'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedBS.sub_df.form.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a329dd9d-1108-4cf8-9ff3-5fb2ded59127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adsh', 'tag', 'version', 'coreg', 'ddate', 'qtrs', 'uom', 'value', 'footnote', 'report', 'line', 'stmt', 'inpth', 'rfile', 'plabel', 'negating'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedBS.pre_num_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "452b18df-500a-448c-9485-71bd44c7717b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['us-gaap/2018', 'us-gaap/2019', 'srt/2019', 'us-gaap/2021',\n",
       "       'srt/2021', 'us-gaap/2022', 'srt/2022', 'us-gaap-sup/2022q3',\n",
       "       'srt-sup/2022q3', 'us-gaap/2023', 'us-gaap/2014', 'us-gaap/2013',\n",
       "       'dei/2013', 'us-gaap/2015', 'us-gaap/2016', 'us-gaap/2017',\n",
       "       'us-gaap/2012', 'dei/2014', 'us-gaap/2020', 'srt/2020',\n",
       "       'us-gaap/2009', 'us-gaap/2011', 'dei/2011', 'us-gaap/2008',\n",
       "       'dei/2012', 'www.sec.gov/20220930', 'srt/2023', 'dei/2019',\n",
       "       'www.sec.gov/20230630'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedBS.pre_num_df.version.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd23fa-0a51-4733-8d85-be5d39e19e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e30588-40df-4acf-af50-f588cd2e0ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdc620-20d7-4d3b-9294-9b1b696241e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f566bf-0422-40dc-8d2f-b2bce2a842c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8b9c1-022a-4a79-b51d-94d6decbe591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
