<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>secfsdstools.u_usecases.bulk_loading API documentation</title>
<meta name="description" content="Contains basic logic that produces separated databags for every of the main financial
statements (BS, CF, IS) containing data from all available zip …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>secfsdstools.u_usecases.bulk_loading</code></h1>
</header>
<section id="section-intro">
<p>Contains basic logic that produces separated databags for every of the main financial
statements (BS, CF, IS) containing data from all available zip files.</p>
<p>The same logic is also contained and further explained in the
06_bulk_data_processing_deep_dive.ipynb notebook.
Please have a look at this notebook for a detailed explanation of the logic</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Contains basic logic that produces separated databags for every of the main financial
statements (BS, CF, IS) containing data from all available zip files.

The same logic is also contained and further explained in the
06_bulk_data_processing_deep_dive.ipynb notebook.
Please have a look at this notebook for a detailed explanation of the logic
&#34;&#34;&#34;
import os
from glob import glob
from typing import Callable, Optional
from typing import List

from secfsdstools.a_config.configmgt import ConfigurationManager
from secfsdstools.c_index.indexdataaccess import ParquetDBIndexingAccessor
from secfsdstools.d_container.databagmodel import RawDataBag, JoinedDataBag
from secfsdstools.e_collector.zipcollecting import ZipCollector


def default_postloadfilter(databag: RawDataBag) -&gt; RawDataBag:
    &#34;&#34;&#34;
    defines a default post filter method that can be used ba ZipCollectors.
    It combines the filters:
        ReportPeriodRawFilter, MainCoregRawFilter, OfficialTagsOnlyRawFilter, USDOnlyRawFilter
    &#34;&#34;&#34;
    # pylint: disable=C0415
    from secfsdstools.e_filter.rawfiltering import ReportPeriodRawFilter, MainCoregRawFilter, \
        OfficialTagsOnlyRawFilter, USDOnlyRawFilter

    return databag[ReportPeriodRawFilter()][MainCoregRawFilter()][OfficialTagsOnlyRawFilter()][
        USDOnlyRawFilter()]


def save_databag(databag: RawDataBag, base_path: str, sub_path: str) -&gt; JoinedDataBag:
    &#34;&#34;&#34;
    helper method to save the RawDataBag and the joined version of it under a certain base_path
    and sub_path.

    the target path for the rawdatabag is &lt;base_path&gt;/&lt;sub_path&gt;/raw.
    the target path for the joineddatabag is &lt;base_path&gt;/&lt;sub_path&gt;/joined.

    Args:
        databag: databag to be saved
        base_path: base path under which the data will be stored
        sub_path: sub path under which the data will be stored

    Returns:
        JoinedDataBag: the joined databag that was created during the save process
    &#34;&#34;&#34;

    target_path_raw = os.path.join(base_path, sub_path, &#39;raw&#39;)
    print(f&#34;store rawdatabag under {target_path_raw}&#34;)
    os.makedirs(target_path_raw, exist_ok=True)
    databag.save(target_path_raw)

    target_path_joined = os.path.join(base_path, sub_path, &#39;joined&#39;)
    os.makedirs(target_path_joined, exist_ok=True)
    print(&#34;create joined databag&#34;)
    joined_databag = databag.join()

    print(f&#34;store joineddatabag under {target_path_joined}&#34;)
    joined_databag.save(target_path_joined)
    return joined_databag


def load_all_financial_statements_parallel(
        financial_statement: str,
        post_load_filter: Optional[Callable[[RawDataBag], RawDataBag]] = None) -&gt; RawDataBag:
    &#34;&#34;&#34;
    loads the data for a certain statement (e.g. BS, CF, IS, ...) from all availalbe zip files
    and returns a single RawDataBag with all information.
    it filters for 10-K and 10-Q reports.

    Args:
        financial_statement (str): the statement you want to read the data for &#34;BS&#34;, &#34;CF&#34;, &#34;IS&#34;
        post_load_filter (Callable, optional): a post_load_filter method that is applied after
         loading of every zip file

    Returns:
        RawDataBag: the databag with the read data

    &#34;&#34;&#34;

    collector: ZipCollector = ZipCollector.get_all_zips(forms_filter=[&#34;10-K&#34;, &#34;10-Q&#34;],
                                                        stmt_filter=[financial_statement],
                                                        post_load_filter=post_load_filter)
    return collector.collect()


def create_datasets_for_main_statements_parallel(base_path: str = &#34;./set/parallel/&#34;):
    &#34;&#34;&#34;
    Creates the raw and joined datasets for all the three main statements: BS, CF, IS.

    The data from the different zip files are loaded in parallel. Therefore, about 16GB
    of free memory is needed.

    the created folder hiearchy looks as follows:
    &lt;pre&gt;
        - &lt;base_path&gt;
          - BS
            - raw
            - joined
          - CF
            - raw
            - joined
          - IS
            - raw
            - joined
    &lt;/pre&gt;
    &#34;&#34;&#34;

    for statement_to_load in [&#34;BS&#34;, &#34;CF&#34;, &#34;IS&#34;]:
        print(&#34;load data for &#34;, statement_to_load)
        rawdatabag = load_all_financial_statements_parallel(
            financial_statement=statement_to_load,
            post_load_filter=default_postloadfilter
        )
        save_databag(databag=rawdatabag, base_path=base_path, sub_path=statement_to_load)


def read_all_zip_names() -&gt; List[str]:
    &#34;&#34;&#34;
    Returns a list with all available zip-file names.

    Returns:
        List[str]: list with the names of the available zip files
    &#34;&#34;&#34;
    configuration = ConfigurationManager.read_config_file()
    dbaccessor = ParquetDBIndexingAccessor(db_dir=configuration.db_dir)

    # exclude 2009q1.zip, since this is empty and causes an error when it is read with a filter
    return [x.fileName for x in dbaccessor.read_all_indexfileprocessing() if
            not x.fullPath.endswith(&#34;2009q1.zip&#34;)]


def build_tmp_set(financial_statement: str,
                  file_names: List[str],
                  base_path: str = &#34;set/tmp/&#34;,
                  post_load_filter: Optional[Callable[[RawDataBag], RawDataBag]] = None):
    &#34;&#34;&#34;
    This function reads the data in sequence from the provided list of zip file names.
    It filters according to the defined financial_statement and stores the data in
    specific subfolders.

    the folder structure will look like
    &lt;target_path&gt;/&lt;file_name&gt;/&lt;financial_statement&gt;/raw
    &lt;target_path&gt;/&lt;file_name&gt;/&lt;financial_statement&gt;/joined

    Args:
        financial_statement (str): the statement you want to read the data for &#34;BS&#34;, &#34;CF&#34;, &#34;IS&#34;
        post_load_filter (Callable, optional): a post_load_filter method that is applied after
         loading of every zip file
        file_names (List[str]): List with the filenames to be processed
        base_path (str): base_path under which the process data is saved.
    &#34;&#34;&#34;

    for file_name in file_names:
        collector = ZipCollector.get_zip_by_name(name=file_name,
                                                 forms_filter=[&#34;10-K&#34;, &#34;10-Q&#34;],
                                                 stmt_filter=[financial_statement],
                                                 post_load_filter=post_load_filter)

        rawdatabag = collector.collect()

        target_path = os.path.join(base_path, file_name)
        # saving the raw databag, joining and saving the joined databag
        save_databag(databag=rawdatabag, base_path=target_path, sub_path=financial_statement)


def create_rawdatabag(financial_statement: str,
                      tmp_path: str = &#34;set/tmp/&#34;,
                      target_path: str = &#34;set/serial/&#34;):
    &#34;&#34;&#34;
    Concatenates the preprocessed and by statement separated rawdatabags into a single databag.

    Args:
        financial_statement: the statement for which data has to be concatenated.
        tmp_path: the path where the temporary files are stored
        target_path: the target path of the daset
    &#34;&#34;&#34;
    raw_files = glob(f&#34;{tmp_path}/*/{financial_statement}/raw/&#34;, recursive=True)
    raw_databags = [RawDataBag.load(file) for file in raw_files]
    raw_databag = RawDataBag.concat(raw_databags)
    target_path_raw = os.path.join(target_path, financial_statement, &#39;raw&#39;)
    print(f&#34;store rawdatabag under {target_path_raw}&#34;)
    os.makedirs(target_path_raw, exist_ok=True)
    raw_databag.save(target_path_raw)


def create_joineddatabag(financial_statement: str,
                         tmp_path: str = &#34;set/tmp/&#34;,
                         target_path: str = &#34;set/serial/&#34;):
    &#34;&#34;&#34;
    Concatenates the preprocessed and by statement separated joineddatabag into a single databag.

    Args:
        financial_statement: the statement for which data has to be concatenated.
        tmp_path: the path where the temporary files are stored
        target_path: the target path of the daset
    &#34;&#34;&#34;

    joined_files = glob(f&#34;{tmp_path}/*/{financial_statement}/joined/&#34;, recursive=True)
    joined_databags = [JoinedDataBag.load(file) for file in joined_files]
    joined_databag = JoinedDataBag.concat(joined_databags)
    target_path_joined = os.path.join(target_path, financial_statement, &#39;joined&#39;)
    print(f&#34;store joineddatabag under {target_path_joined}&#34;)
    os.makedirs(target_path_joined, exist_ok=True)
    joined_databag.save(target_path_joined)


def create_datasets_for_main_statements_serial(target_path: str = &#34;set/parallel/&#34;,
                                               tmp_path: str = &#34;set/tmp&#34;):
    &#34;&#34;&#34;
    Creates the rawdatabag and joineddatabag for all three main financial statements (BS, CF, IS).

    It is done in serial manner, and therefore needs less resources than the parallel approach.

    the created folder hiearchy looks as follows:
    &lt;pre&gt;
        - &lt;target_path&gt;
          - BS
            - raw
            - joined
          - CF
            - raw
            - joined
          - IS
            - raw
            - joined
    &lt;/pre&gt;
    &#34;&#34;&#34;
    file_names = read_all_zip_names()

    # create the temporary datasets
    # Note: calling the build_tmp_set doesn&#39;t needs a lot of memory
    build_tmp_set(financial_statement=&#34;BS&#34;, file_names=file_names,
                  post_load_filter=default_postloadfilter, base_path=tmp_path)
    build_tmp_set(financial_statement=&#34;IS&#34;, file_names=file_names,
                  post_load_filter=default_postloadfilter, base_path=tmp_path)
    build_tmp_set(financial_statement=&#34;CF&#34;, file_names=file_names,
                  post_load_filter=default_postloadfilter, base_path=tmp_path)

    # Note: calling the create_rawdatabag needs about 8-10 GB of free memory.
    #       the memory should be garbage collected between the different create_rawdatabag calls
    create_rawdatabag(financial_statement=&#34;BS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_rawdatabag(financial_statement=&#34;IS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_rawdatabag(financial_statement=&#34;CF&#34;, target_path=target_path, tmp_path=tmp_path)

    # Note: calling the create_joinedatabag needs about 4 GB of free memory.
    #       the memory should be garbage collected between the different create_joineddatabag calls
    create_joineddatabag(financial_statement=&#34;BS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_joineddatabag(financial_statement=&#34;IS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_joineddatabag(financial_statement=&#34;CF&#34;, target_path=target_path, tmp_path=tmp_path)


if __name__ == &#39;__main__&#39;:
    print(&#34;depending on your hardware resources, run the parallel or the serial logic.&#34;,
          &#34;Just uncommented the desired line of code..&#34;)
    # create_datasets_for_main_statements_parallel()
    # create_datasets_for_main_statements_serial()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="secfsdstools.u_usecases.bulk_loading.build_tmp_set"><code class="name flex">
<span>def <span class="ident">build_tmp_set</span></span>(<span>financial_statement: str, file_names: List[str], base_path: str = 'set/tmp/', post_load_filter: Optional[Callable[[<a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a>], <a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a>]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>This function reads the data in sequence from the provided list of zip file names.
It filters according to the defined financial_statement and stores the data in
specific subfolders.</p>
<p>the folder structure will look like
<target_path>/<file_name>/<financial_statement>/raw
<target_path>/<file_name>/<financial_statement>/joined</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>financial_statement</code></strong> :&ensp;<code>str</code></dt>
<dd>the statement you want to read the data for "BS", "CF", "IS"</dd>
<dt><strong><code>post_load_filter</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>a post_load_filter method that is applied after</dd>
<dt>loading of every zip file</dt>
<dt><strong><code>file_names</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List with the filenames to be processed</dd>
<dt><strong><code>base_path</code></strong> :&ensp;<code>str</code></dt>
<dd>base_path under which the process data is saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_tmp_set(financial_statement: str,
                  file_names: List[str],
                  base_path: str = &#34;set/tmp/&#34;,
                  post_load_filter: Optional[Callable[[RawDataBag], RawDataBag]] = None):
    &#34;&#34;&#34;
    This function reads the data in sequence from the provided list of zip file names.
    It filters according to the defined financial_statement and stores the data in
    specific subfolders.

    the folder structure will look like
    &lt;target_path&gt;/&lt;file_name&gt;/&lt;financial_statement&gt;/raw
    &lt;target_path&gt;/&lt;file_name&gt;/&lt;financial_statement&gt;/joined

    Args:
        financial_statement (str): the statement you want to read the data for &#34;BS&#34;, &#34;CF&#34;, &#34;IS&#34;
        post_load_filter (Callable, optional): a post_load_filter method that is applied after
         loading of every zip file
        file_names (List[str]): List with the filenames to be processed
        base_path (str): base_path under which the process data is saved.
    &#34;&#34;&#34;

    for file_name in file_names:
        collector = ZipCollector.get_zip_by_name(name=file_name,
                                                 forms_filter=[&#34;10-K&#34;, &#34;10-Q&#34;],
                                                 stmt_filter=[financial_statement],
                                                 post_load_filter=post_load_filter)

        rawdatabag = collector.collect()

        target_path = os.path.join(base_path, file_name)
        # saving the raw databag, joining and saving the joined databag
        save_databag(databag=rawdatabag, base_path=target_path, sub_path=financial_statement)</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.create_datasets_for_main_statements_parallel"><code class="name flex">
<span>def <span class="ident">create_datasets_for_main_statements_parallel</span></span>(<span>base_path: str = './set/parallel/')</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the raw and joined datasets for all the three main statements: BS, CF, IS.</p>
<p>The data from the different zip files are loaded in parallel. Therefore, about 16GB
of free memory is needed.</p>
<p>the created folder hiearchy looks as follows:</p>
<pre>
    - <base_path>
      - BS
        - raw
        - joined
      - CF
        - raw
        - joined
      - IS
        - raw
        - joined
</pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_datasets_for_main_statements_parallel(base_path: str = &#34;./set/parallel/&#34;):
    &#34;&#34;&#34;
    Creates the raw and joined datasets for all the three main statements: BS, CF, IS.

    The data from the different zip files are loaded in parallel. Therefore, about 16GB
    of free memory is needed.

    the created folder hiearchy looks as follows:
    &lt;pre&gt;
        - &lt;base_path&gt;
          - BS
            - raw
            - joined
          - CF
            - raw
            - joined
          - IS
            - raw
            - joined
    &lt;/pre&gt;
    &#34;&#34;&#34;

    for statement_to_load in [&#34;BS&#34;, &#34;CF&#34;, &#34;IS&#34;]:
        print(&#34;load data for &#34;, statement_to_load)
        rawdatabag = load_all_financial_statements_parallel(
            financial_statement=statement_to_load,
            post_load_filter=default_postloadfilter
        )
        save_databag(databag=rawdatabag, base_path=base_path, sub_path=statement_to_load)</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.create_datasets_for_main_statements_serial"><code class="name flex">
<span>def <span class="ident">create_datasets_for_main_statements_serial</span></span>(<span>target_path: str = 'set/parallel/', tmp_path: str = 'set/tmp')</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the rawdatabag and joineddatabag for all three main financial statements (BS, CF, IS).</p>
<p>It is done in serial manner, and therefore needs less resources than the parallel approach.</p>
<p>the created folder hiearchy looks as follows:</p>
<pre>
    - <target_path>
      - BS
        - raw
        - joined
      - CF
        - raw
        - joined
      - IS
        - raw
        - joined
</pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_datasets_for_main_statements_serial(target_path: str = &#34;set/parallel/&#34;,
                                               tmp_path: str = &#34;set/tmp&#34;):
    &#34;&#34;&#34;
    Creates the rawdatabag and joineddatabag for all three main financial statements (BS, CF, IS).

    It is done in serial manner, and therefore needs less resources than the parallel approach.

    the created folder hiearchy looks as follows:
    &lt;pre&gt;
        - &lt;target_path&gt;
          - BS
            - raw
            - joined
          - CF
            - raw
            - joined
          - IS
            - raw
            - joined
    &lt;/pre&gt;
    &#34;&#34;&#34;
    file_names = read_all_zip_names()

    # create the temporary datasets
    # Note: calling the build_tmp_set doesn&#39;t needs a lot of memory
    build_tmp_set(financial_statement=&#34;BS&#34;, file_names=file_names,
                  post_load_filter=default_postloadfilter, base_path=tmp_path)
    build_tmp_set(financial_statement=&#34;IS&#34;, file_names=file_names,
                  post_load_filter=default_postloadfilter, base_path=tmp_path)
    build_tmp_set(financial_statement=&#34;CF&#34;, file_names=file_names,
                  post_load_filter=default_postloadfilter, base_path=tmp_path)

    # Note: calling the create_rawdatabag needs about 8-10 GB of free memory.
    #       the memory should be garbage collected between the different create_rawdatabag calls
    create_rawdatabag(financial_statement=&#34;BS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_rawdatabag(financial_statement=&#34;IS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_rawdatabag(financial_statement=&#34;CF&#34;, target_path=target_path, tmp_path=tmp_path)

    # Note: calling the create_joinedatabag needs about 4 GB of free memory.
    #       the memory should be garbage collected between the different create_joineddatabag calls
    create_joineddatabag(financial_statement=&#34;BS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_joineddatabag(financial_statement=&#34;IS&#34;, target_path=target_path, tmp_path=tmp_path)
    create_joineddatabag(financial_statement=&#34;CF&#34;, target_path=target_path, tmp_path=tmp_path)</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.create_joineddatabag"><code class="name flex">
<span>def <span class="ident">create_joineddatabag</span></span>(<span>financial_statement: str, tmp_path: str = 'set/tmp/', target_path: str = 'set/serial/')</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates the preprocessed and by statement separated joineddatabag into a single databag.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>financial_statement</code></strong></dt>
<dd>the statement for which data has to be concatenated.</dd>
<dt><strong><code>tmp_path</code></strong></dt>
<dd>the path where the temporary files are stored</dd>
<dt><strong><code>target_path</code></strong></dt>
<dd>the target path of the daset</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_joineddatabag(financial_statement: str,
                         tmp_path: str = &#34;set/tmp/&#34;,
                         target_path: str = &#34;set/serial/&#34;):
    &#34;&#34;&#34;
    Concatenates the preprocessed and by statement separated joineddatabag into a single databag.

    Args:
        financial_statement: the statement for which data has to be concatenated.
        tmp_path: the path where the temporary files are stored
        target_path: the target path of the daset
    &#34;&#34;&#34;

    joined_files = glob(f&#34;{tmp_path}/*/{financial_statement}/joined/&#34;, recursive=True)
    joined_databags = [JoinedDataBag.load(file) for file in joined_files]
    joined_databag = JoinedDataBag.concat(joined_databags)
    target_path_joined = os.path.join(target_path, financial_statement, &#39;joined&#39;)
    print(f&#34;store joineddatabag under {target_path_joined}&#34;)
    os.makedirs(target_path_joined, exist_ok=True)
    joined_databag.save(target_path_joined)</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.create_rawdatabag"><code class="name flex">
<span>def <span class="ident">create_rawdatabag</span></span>(<span>financial_statement: str, tmp_path: str = 'set/tmp/', target_path: str = 'set/serial/')</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates the preprocessed and by statement separated rawdatabags into a single databag.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>financial_statement</code></strong></dt>
<dd>the statement for which data has to be concatenated.</dd>
<dt><strong><code>tmp_path</code></strong></dt>
<dd>the path where the temporary files are stored</dd>
<dt><strong><code>target_path</code></strong></dt>
<dd>the target path of the daset</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_rawdatabag(financial_statement: str,
                      tmp_path: str = &#34;set/tmp/&#34;,
                      target_path: str = &#34;set/serial/&#34;):
    &#34;&#34;&#34;
    Concatenates the preprocessed and by statement separated rawdatabags into a single databag.

    Args:
        financial_statement: the statement for which data has to be concatenated.
        tmp_path: the path where the temporary files are stored
        target_path: the target path of the daset
    &#34;&#34;&#34;
    raw_files = glob(f&#34;{tmp_path}/*/{financial_statement}/raw/&#34;, recursive=True)
    raw_databags = [RawDataBag.load(file) for file in raw_files]
    raw_databag = RawDataBag.concat(raw_databags)
    target_path_raw = os.path.join(target_path, financial_statement, &#39;raw&#39;)
    print(f&#34;store rawdatabag under {target_path_raw}&#34;)
    os.makedirs(target_path_raw, exist_ok=True)
    raw_databag.save(target_path_raw)</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.default_postloadfilter"><code class="name flex">
<span>def <span class="ident">default_postloadfilter</span></span>(<span>databag: <a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a>) ‑> <a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a></span>
</code></dt>
<dd>
<div class="desc"><p>defines a default post filter method that can be used ba ZipCollectors.
It combines the filters:
ReportPeriodRawFilter, MainCoregRawFilter, OfficialTagsOnlyRawFilter, USDOnlyRawFilter</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def default_postloadfilter(databag: RawDataBag) -&gt; RawDataBag:
    &#34;&#34;&#34;
    defines a default post filter method that can be used ba ZipCollectors.
    It combines the filters:
        ReportPeriodRawFilter, MainCoregRawFilter, OfficialTagsOnlyRawFilter, USDOnlyRawFilter
    &#34;&#34;&#34;
    # pylint: disable=C0415
    from secfsdstools.e_filter.rawfiltering import ReportPeriodRawFilter, MainCoregRawFilter, \
        OfficialTagsOnlyRawFilter, USDOnlyRawFilter

    return databag[ReportPeriodRawFilter()][MainCoregRawFilter()][OfficialTagsOnlyRawFilter()][
        USDOnlyRawFilter()]</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.load_all_financial_statements_parallel"><code class="name flex">
<span>def <span class="ident">load_all_financial_statements_parallel</span></span>(<span>financial_statement: str, post_load_filter: Optional[Callable[[<a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a>], <a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a>]] = None) ‑> <a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a></span>
</code></dt>
<dd>
<div class="desc"><p>loads the data for a certain statement (e.g. BS, CF, IS, &hellip;) from all availalbe zip files
and returns a single RawDataBag with all information.
it filters for 10-K and 10-Q reports.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>financial_statement</code></strong> :&ensp;<code>str</code></dt>
<dd>the statement you want to read the data for "BS", "CF", "IS"</dd>
<dt><strong><code>post_load_filter</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>a post_load_filter method that is applied after</dd>
</dl>
<p>loading of every zip file</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>RawDataBag</code></dt>
<dd>the databag with the read data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_all_financial_statements_parallel(
        financial_statement: str,
        post_load_filter: Optional[Callable[[RawDataBag], RawDataBag]] = None) -&gt; RawDataBag:
    &#34;&#34;&#34;
    loads the data for a certain statement (e.g. BS, CF, IS, ...) from all availalbe zip files
    and returns a single RawDataBag with all information.
    it filters for 10-K and 10-Q reports.

    Args:
        financial_statement (str): the statement you want to read the data for &#34;BS&#34;, &#34;CF&#34;, &#34;IS&#34;
        post_load_filter (Callable, optional): a post_load_filter method that is applied after
         loading of every zip file

    Returns:
        RawDataBag: the databag with the read data

    &#34;&#34;&#34;

    collector: ZipCollector = ZipCollector.get_all_zips(forms_filter=[&#34;10-K&#34;, &#34;10-Q&#34;],
                                                        stmt_filter=[financial_statement],
                                                        post_load_filter=post_load_filter)
    return collector.collect()</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.read_all_zip_names"><code class="name flex">
<span>def <span class="ident">read_all_zip_names</span></span>(<span>) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a list with all available zip-file names.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>list with the names of the available zip files</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_all_zip_names() -&gt; List[str]:
    &#34;&#34;&#34;
    Returns a list with all available zip-file names.

    Returns:
        List[str]: list with the names of the available zip files
    &#34;&#34;&#34;
    configuration = ConfigurationManager.read_config_file()
    dbaccessor = ParquetDBIndexingAccessor(db_dir=configuration.db_dir)

    # exclude 2009q1.zip, since this is empty and causes an error when it is read with a filter
    return [x.fileName for x in dbaccessor.read_all_indexfileprocessing() if
            not x.fullPath.endswith(&#34;2009q1.zip&#34;)]</code></pre>
</details>
</dd>
<dt id="secfsdstools.u_usecases.bulk_loading.save_databag"><code class="name flex">
<span>def <span class="ident">save_databag</span></span>(<span>databag: <a title="secfsdstools.d_container.databagmodel.RawDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.RawDataBag">RawDataBag</a>, base_path: str, sub_path: str) ‑> <a title="secfsdstools.d_container.databagmodel.JoinedDataBag" href="../d_container/databagmodel.html#secfsdstools.d_container.databagmodel.JoinedDataBag">JoinedDataBag</a></span>
</code></dt>
<dd>
<div class="desc"><p>helper method to save the RawDataBag and the joined version of it under a certain base_path
and sub_path.</p>
<p>the target path for the rawdatabag is <base_path>/<sub_path>/raw.
the target path for the joineddatabag is <base_path>/<sub_path>/joined.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>databag</code></strong></dt>
<dd>databag to be saved</dd>
<dt><strong><code>base_path</code></strong></dt>
<dd>base path under which the data will be stored</dd>
<dt><strong><code>sub_path</code></strong></dt>
<dd>sub path under which the data will be stored</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>JoinedDataBag</code></dt>
<dd>the joined databag that was created during the save process</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_databag(databag: RawDataBag, base_path: str, sub_path: str) -&gt; JoinedDataBag:
    &#34;&#34;&#34;
    helper method to save the RawDataBag and the joined version of it under a certain base_path
    and sub_path.

    the target path for the rawdatabag is &lt;base_path&gt;/&lt;sub_path&gt;/raw.
    the target path for the joineddatabag is &lt;base_path&gt;/&lt;sub_path&gt;/joined.

    Args:
        databag: databag to be saved
        base_path: base path under which the data will be stored
        sub_path: sub path under which the data will be stored

    Returns:
        JoinedDataBag: the joined databag that was created during the save process
    &#34;&#34;&#34;

    target_path_raw = os.path.join(base_path, sub_path, &#39;raw&#39;)
    print(f&#34;store rawdatabag under {target_path_raw}&#34;)
    os.makedirs(target_path_raw, exist_ok=True)
    databag.save(target_path_raw)

    target_path_joined = os.path.join(base_path, sub_path, &#39;joined&#39;)
    os.makedirs(target_path_joined, exist_ok=True)
    print(&#34;create joined databag&#34;)
    joined_databag = databag.join()

    print(f&#34;store joineddatabag under {target_path_joined}&#34;)
    joined_databag.save(target_path_joined)
    return joined_databag</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="secfsdstools.u_usecases" href="index.html">secfsdstools.u_usecases</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="secfsdstools.u_usecases.bulk_loading.build_tmp_set" href="#secfsdstools.u_usecases.bulk_loading.build_tmp_set">build_tmp_set</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.create_datasets_for_main_statements_parallel" href="#secfsdstools.u_usecases.bulk_loading.create_datasets_for_main_statements_parallel">create_datasets_for_main_statements_parallel</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.create_datasets_for_main_statements_serial" href="#secfsdstools.u_usecases.bulk_loading.create_datasets_for_main_statements_serial">create_datasets_for_main_statements_serial</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.create_joineddatabag" href="#secfsdstools.u_usecases.bulk_loading.create_joineddatabag">create_joineddatabag</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.create_rawdatabag" href="#secfsdstools.u_usecases.bulk_loading.create_rawdatabag">create_rawdatabag</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.default_postloadfilter" href="#secfsdstools.u_usecases.bulk_loading.default_postloadfilter">default_postloadfilter</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.load_all_financial_statements_parallel" href="#secfsdstools.u_usecases.bulk_loading.load_all_financial_statements_parallel">load_all_financial_statements_parallel</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.read_all_zip_names" href="#secfsdstools.u_usecases.bulk_loading.read_all_zip_names">read_all_zip_names</a></code></li>
<li><code><a title="secfsdstools.u_usecases.bulk_loading.save_databag" href="#secfsdstools.u_usecases.bulk_loading.save_databag">save_databag</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>