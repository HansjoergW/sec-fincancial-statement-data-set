<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>secfsdstools.x_examples.automation.memory_optimized_daily_automation API documentation</title>
<meta name="description" content="This module shows the automation to add additional steps after the usual update process
(which is downloading new zip files, transforming them to …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>secfsdstools.x_examples.automation.memory_optimized_daily_automation</code></h1>
</header>
<section id="section-intro">
<p>This module shows the automation to add additional steps after the usual update process
(which is downloading new zip files, transforming them to parquet, indexding them).</p>
<p>This example has a low memory footprint, so it is safe to use also on systems with 16GB
of memory. It also activates daily processing.</p>
<p>If you update the configuration as defined below, you will get the following datasets:</p>
<ul>
<li>
<p>for quarterly data provided and parsed by the SEC:</p>
<ul>
<li>single filtered and joined bags for every stmt (BS, IS, CF, ..) containing the data from
all available zip files.</li>
<li>a single filtered and joined bag containing the data from all available zip files.</li>
<li>single standardized bags for BS, IS, CF containing the data from all available zip files.</li>
</ul>
</li>
<li>
<p>for daily data provided by the SEC and parsed locally with the <code>secdaily</code> package
(not yet available as quaterly data):</p>
<ul>
<li>single filtered and joined bags for every stmt (BS, IS, CF, ..) containing daily parsed data.</li>
<li>a single filtered and joined bag containing the data from all daily parsed data.</li>
<li>single standardized bags for BS, IS, CF containing the data from all daily parsed data.</li>
</ul>
</li>
<li>
<p>combined quarterly and daily data:</p>
<ul>
<li>single filtered and joined bags for every stmt (BS, IS, CF, ..) from all available data</li>
<li>a single filtered and joined bag containing the data from all available data.</li>
<li>single standardized bags for BS, IS, CF containing the data from all available data.</li>
</ul>
</li>
</ul>
<p>Moreover, these files will be automatically updated as soon as a new quarterly zip file or a new daily data
becomes available at the SEC website.</p>
<p>You can configure this function in the secfsdstools configuration file, by adding
a postupdateprocesses definition. For instance, if you want to use this example,
just add the postupdateprocesses definition as shown below:</p>
<pre>
[DEFAULT]
downloaddirectory = ...
dbdirectory = ...
parquetdirectory = ...
useragentemail = ...
autoupdate = True
keepzipfiles = False

postupdateprocesses=secfsdstools.x_examples.automation.memory_optimized_daily_automation.define_extra_processes

# activate daily processing
dailyprocessing = True

</pre>
<p>If you want to use it, you also need to add additional configuration entries as shown below:</p>
<pre>
[Filter]
filtered_quarterly_joined_by_stmt_dir = C:/data/sec/automated/_1_by_quarter/_1_filtered_joined_by_stmt
filtered_daily_joined_by_stmt_dir = C:/data/sec/automated/_1_by_day/_1_filtered_joined_by_stmt
parallelize = True

[Standardizer]
standardized_quarterly_by_stmt_dir = C:/data/sec/automated/_1_by_quarter/_2_standardized_by_stmt
standardized_daily_by_stmt_dir = C:/data/sec/automated/_1_by_day/_2_standardized_by_stmt

[Concat]
concat_quarterly_joined_by_stmt_dir = C:/data/sec/automated/_2_all_quarter/_1_joined_by_stmt
concat_daily_joined_by_stmt_dir = C:/data/sec/automated/_2_all_day/_1_joined_by_stmt

concat_quarterly_joined_all_dir = C:/data/sec/automated/_2_all_quarter/_2_joined
concat_daily_joined_all_dir = C:/data/sec/automated/_2_all_day/_2_joined

concat_quarterly_standardized_by_stmt_dir = C:/data/sec/automated/_2_all_quarter/_3_standardized_by_stmt
concat_daily_standardized_by_stmt_dir = C:/data/sec/automated/_2_all_day/_3_standardized_by_stmt

concat_all_joined_by_stmt_dir = C:/data/sec/automated/_3_all/_1_joined_by_stmt
concat_all_joined_dir = C:/data/sec/automated/_3_all/_2_joined
concat_all_standardized_by_stmt_dir = C:/data/sec/automated/_3_all/_3_standardized_by_stmt

</pre>
<p>(A complete configuration file using the "define_extra_processes" function is available in the file
memory_optimized_daily_automation_config.cfg which is in the same package as this module here.)</p>
<p>This example adds the following main steps to the usual update process.</p>
<ul>
<li>
<p>for quarterly data provided and parsed by the SEC, as well as daily data not yet part of the quarterly data:</p>
<p>First, it creates a joined bag for every zip file, filters it for 10-K and 10-Q reports only
and also applies the filters
ReportPeriodRawFilter, MainCoregRawFilter, USDOnlyRawFilter,
OfficialTagsOnlyRawFilter. Furthermore, the data will also be split by stmt.
Location: filtered_[quarter/daily]_joined_by_stmt_dir.
Note: setting "parallelize" in the config to False, will be slower in the initial loading
but using less memory.</p>
<p>Second, it produces standardized bags for BS, IS, CF for every zip file based on the filtered
data from the previous step. These bags are stored under the path defined as
<code>standardized_[quarterly/daily]_by_stmt_dir</code>.</p>
<p>Third, it creates a single joined bag for every statement (balance sheet, income statement,
cash flow, &hellip;) based on the filtered data from the first step.
These bags are stored under the path defined as <code>concat_[quarterly/daily]_joined_by_stmt_dir</code>.</p>
<p>Fourth, it will create a single bag with all data from all different statements, by merging the
bags from the previous step into a single big bag. This bag will be stored under the path defined
as <code>concat_[quarterly,daily]_joined_all_dir</code>.</p>
<p>Fifth, it will create single standardized bags for BS, IS, CF from the results in the second step.
These bags will be stored under the path defined as <code>concat_[quarterly/daily]_standardized_by_stmt_dir</code>.</p>
</li>
<li>
<p>combined quarterly and daily data:</p>
<p>First, it creates a single joined bag for every statement (balance sheet, income statement,
cash flow, &hellip;) containing quarterly and daily data.
These bags are stored under the path defined as <code>concat_all_joined_by_stmt_dir</code>.</p>
<p>Second, it will create a single bag with all data from all different statements, by merging the
bags from the previous step into a single big bag. This bag will be stored under the path defined
as <code>concat_all_joined_all_dir</code>.</p>
<p>Third, it will create single standardized bags for BS, IS, CF containing data from both quarterly and daily data.
These bags will be stored under the path defined as <code>concat_all_standardized_by_stmt_dir</code>.</p>
</li>
</ul>
<p>All this steps use basic implementations of the AbstractProcess class from the
secfsdstools.g_pipeline package.</p>
<p>Furthermore, all these steps check if something changed since the last run and are only executed
if something did change (for instance, if a new data became available).</p>
<p>Have also a look at the notebook 08_00_automation_basics.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># pylint: disable=C0301
&#34;&#34;&#34;
This module shows the automation to add additional steps after the usual update process
(which is downloading new zip files, transforming them to parquet, indexding them).

This example has a low memory footprint, so it is safe to use also on systems with 16GB
of memory. It also activates daily processing.

If you update the configuration as defined below, you will get the following datasets:

- for quarterly data provided and parsed by the SEC:
    - single filtered and joined bags for every stmt (BS, IS, CF, ..) containing the data from
      all available zip files.
    - a single filtered and joined bag containing the data from all available zip files.
    - single standardized bags for BS, IS, CF containing the data from all available zip files.

- for daily data provided by the SEC and parsed locally with the `secdaily` package
  (not yet available as quaterly data):
    - single filtered and joined bags for every stmt (BS, IS, CF, ..) containing daily parsed data.
    - a single filtered and joined bag containing the data from all daily parsed data.
    - single standardized bags for BS, IS, CF containing the data from all daily parsed data.

- combined quarterly and daily data:
    - single filtered and joined bags for every stmt (BS, IS, CF, ..) from all available data
    - a single filtered and joined bag containing the data from all available data.
    - single standardized bags for BS, IS, CF containing the data from all available data.


Moreover, these files will be automatically updated as soon as a new quarterly zip file or a new daily data
becomes available at the SEC website.

You can configure this function in the secfsdstools configuration file, by adding
a postupdateprocesses definition. For instance, if you want to use this example,
just add the postupdateprocesses definition as shown below:

&lt;pre&gt;
[DEFAULT]
downloaddirectory = ...
dbdirectory = ...
parquetdirectory = ...
useragentemail = ...
autoupdate = True
keepzipfiles = False

postupdateprocesses=secfsdstools.x_examples.automation.memory_optimized_daily_automation.define_extra_processes

# activate daily processing
dailyprocessing = True

&lt;/pre&gt;

If you want to use it, you also need to add additional configuration entries as shown below:

&lt;pre&gt;
[Filter]
filtered_quarterly_joined_by_stmt_dir = C:/data/sec/automated/_1_by_quarter/_1_filtered_joined_by_stmt
filtered_daily_joined_by_stmt_dir = C:/data/sec/automated/_1_by_day/_1_filtered_joined_by_stmt
parallelize = True

[Standardizer]
standardized_quarterly_by_stmt_dir = C:/data/sec/automated/_1_by_quarter/_2_standardized_by_stmt
standardized_daily_by_stmt_dir = C:/data/sec/automated/_1_by_day/_2_standardized_by_stmt

[Concat]
concat_quarterly_joined_by_stmt_dir = C:/data/sec/automated/_2_all_quarter/_1_joined_by_stmt
concat_daily_joined_by_stmt_dir = C:/data/sec/automated/_2_all_day/_1_joined_by_stmt

concat_quarterly_joined_all_dir = C:/data/sec/automated/_2_all_quarter/_2_joined
concat_daily_joined_all_dir = C:/data/sec/automated/_2_all_day/_2_joined

concat_quarterly_standardized_by_stmt_dir = C:/data/sec/automated/_2_all_quarter/_3_standardized_by_stmt
concat_daily_standardized_by_stmt_dir = C:/data/sec/automated/_2_all_day/_3_standardized_by_stmt

concat_all_joined_by_stmt_dir = C:/data/sec/automated/_3_all/_1_joined_by_stmt
concat_all_joined_dir = C:/data/sec/automated/_3_all/_2_joined
concat_all_standardized_by_stmt_dir = C:/data/sec/automated/_3_all/_3_standardized_by_stmt

&lt;/pre&gt;

(A complete configuration file using the &#34;define_extra_processes&#34; function is available in the file
 memory_optimized_daily_automation_config.cfg which is in the same package as this module here.)

This example adds the following main steps to the usual update process.

- for quarterly data provided and parsed by the SEC, as well as daily data not yet part of the quarterly data:

    First, it creates a joined bag for every zip file, filters it for 10-K and 10-Q reports only
    and also applies the filters  ReportPeriodRawFilter, MainCoregRawFilter, USDOnlyRawFilter,
    OfficialTagsOnlyRawFilter. Furthermore, the data will also be split by stmt.
    Location: filtered_[quarter/daily]_joined_by_stmt_dir.
    Note: setting &#34;parallelize&#34; in the config to False, will be slower in the initial loading
    but using less memory.

    Second, it produces standardized bags for BS, IS, CF for every zip file based on the filtered
    data from the previous step. These bags are stored under the path defined as
    `standardized_[quarterly/daily]_by_stmt_dir`.

    Third, it creates a single joined bag for every statement (balance sheet, income statement,
    cash flow, ...) based on the filtered data from the first step.
    These bags are stored under the path defined as `concat_[quarterly/daily]_joined_by_stmt_dir`.

    Fourth, it will create a single bag with all data from all different statements, by merging the
    bags from the previous step into a single big bag. This bag will be stored under the path defined
    as `concat_[quarterly,daily]_joined_all_dir`.

    Fifth, it will create single standardized bags for BS, IS, CF from the results in the second step.
    These bags will be stored under the path defined as `concat_[quarterly/daily]_standardized_by_stmt_dir`.

- combined quarterly and daily data:

    First, it creates a single joined bag for every statement (balance sheet, income statement,
    cash flow, ...) containing quarterly and daily data.
    These bags are stored under the path defined as `concat_all_joined_by_stmt_dir`.

    Second, it will create a single bag with all data from all different statements, by merging the
    bags from the previous step into a single big bag. This bag will be stored under the path defined
    as `concat_all_joined_all_dir`.

    Third, it will create single standardized bags for BS, IS, CF containing data from both quarterly and daily data.
    These bags will be stored under the path defined as `concat_all_standardized_by_stmt_dir`.


All this steps use basic implementations of the AbstractProcess class from the
secfsdstools.g_pipeline package.

Furthermore, all these steps check if something changed since the last run and are only executed
if something did change (for instance, if a new data became available).

Have also a look at the notebook 08_00_automation_basics.

&#34;&#34;&#34;
import shutil
from pathlib import Path
from typing import List

from secfsdstools.a_config.configmodel import Configuration
from secfsdstools.c_automation.task_framework import AbstractProcess, LoggingProcess
from secfsdstools.c_daily.dailypreparation_process import DailyPreparationProcess
from secfsdstools.c_index.indexdataaccess import ParquetDBIndexingAccessor
from secfsdstools.g_pipelines.concat_process import (
    ConcatByChangedTimestampProcess,
    ConcatByNewSubfoldersProcess,
    ConcatMultiRootByChangedTimestampProcess,
)
from secfsdstools.g_pipelines.filter_process import FilterProcess
from secfsdstools.g_pipelines.standardize_process import StandardizeProcess


class ClearDailyDataProcess(AbstractProcess):
    &#34;&#34;&#34;
    Since daily processed data is removed, as soon as the data is contained in a quarterly zip file,
    we also need to remove the according data from the automated datasets.

    This process does this for the filtered and standardized daily data, which are created in the first step
    of processing the daily data.

    The following steps for the daily processing always creates the dataset fresh from all available daily data.
    Hence, we only need to clean the filtered and standardized daily data.
    &#34;&#34;&#34;

    def __init__(self, db_dir: str, filtered_daily_joined_by_stmt_dir: str, standardized_daily_by_stmt_dir: str):
        &#34;&#34;&#34;
        Constructor.
        Args:
            db_dir: directory of the database
            filtered_daily_joined_by_stmt_dir: directory containing the filtered daily data
            standardized_daily_by_stmt_dir: directory containing the standardized daily data
        &#34;&#34;&#34;
        super().__init__()
        self.db_dir = db_dir
        self.index_accessor = ParquetDBIndexingAccessor(db_dir=db_dir)
        self.filtered_daily_joined_by_stmt_dir = filtered_daily_joined_by_stmt_dir
        self.standardized_daily_by_stmt_dir = standardized_daily_by_stmt_dir

    def clear_directory(self, cut_off_day: int, root_dir_path: Path):
        &#34;&#34;&#34;
        removes all directories under root_dir_path that are older than the cut_off_day.
        &#34;&#34;&#34;

        cut_off_file_name = f&#34;{cut_off_day}.zip&#34;

        if root_dir_path.exists():
            for dir_path in root_dir_path.iterdir():
                if dir_path.is_dir() and dir_path.name &lt; cut_off_file_name:
                    shutil.rmtree(dir_path)

    def process(self):
        &#34;&#34;&#34;
        Execute the process.
        &#34;&#34;&#34;
        last_processed_quarter_file_name = self.index_accessor.find_latest_quarter_file_name()
        if last_processed_quarter_file_name is None:
            raise ValueError(
                &#34;No quarterly files were processed before. &#34;
                &#34;Please process quarterly files first before running the daily process.&#34;
            )
        last_processed_quarter = last_processed_quarter_file_name.split(&#34;.&#34;)[0]

        daily_start_quarter = DailyPreparationProcess.calculate_daily_start_quarter(last_processed_quarter)
        cut_off_day = DailyPreparationProcess.cut_off_day(daily_start_quarter)
        self.clear_directory(
            cut_off_day=cut_off_day, root_dir_path=Path(self.filtered_daily_joined_by_stmt_dir) / &#34;daily&#34;
        )
        self.clear_directory(cut_off_day=cut_off_day, root_dir_path=Path(self.standardized_daily_by_stmt_dir))


def define_extra_processes(configuration: Configuration) -&gt; List[AbstractProcess]:
    &#34;&#34;&#34;
    example definition of an additional pipeline. It adds sevreal steps to process
    quarterly and daily data, as well as combining both. See the documentation of this
    module for more details.

    All these steps have a low memory footprint, so, the should run without any problems also
    on 16 GB machine.

    Please have a look at the notebook 08_00_automation_basics for further details.

    Args:
        configuration: the configuration

    Returns:
        List[AbstractProcess]: List with the defined process steps

    &#34;&#34;&#34;

    filtered_quarterly_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Filter&#34;, option=&#34;filtered_quarterly_joined_by_stmt_dir&#34;
    )
    filtered_daily_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Filter&#34;, option=&#34;filtered_daily_joined_by_stmt_dir&#34;
    )

    filter_parallelize = configuration.get_parser().get(section=&#34;Filter&#34;, option=&#34;parallelize&#34;, fallback=True)

    standardized_quarterly_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Standardizer&#34;, option=&#34;standardized_quarterly_by_stmt_dir&#34;
    )
    standardized_daily_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Standardizer&#34;, option=&#34;standardized_daily_by_stmt_dir&#34;
    )

    concat_quarterly_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_quarterly_joined_by_stmt_dir&#34;
    )
    concat_daily_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_daily_joined_by_stmt_dir&#34;
    )

    concat_quarterly_joined_all_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_quarterly_joined_all_dir&#34;
    )
    concat_daily_joined_all_dir = configuration.get_parser().get(section=&#34;Concat&#34;, option=&#34;concat_daily_joined_all_dir&#34;)

    concat_quarterly_standardized_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_quarterly_standardized_by_stmt_dir&#34;
    )
    concat_daily_standardized_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_daily_standardized_by_stmt_dir&#34;
    )

    concat_all_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_all_joined_by_stmt_dir&#34;
    )

    concat_all_joined_dir = configuration.get_parser().get(section=&#34;Concat&#34;, option=&#34;concat_all_joined_dir&#34;)

    concat_all_standardized_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_all_standardized_by_stmt_dir&#34;
    )

    processes: List[AbstractProcess] = []

    # QUARTERLY DATA Processing
    processes.append(LoggingProcess(title=&#34;Post Update Processes For Quarterly Data Started&#34;, lines=[]))

    processes.append(
        # 1. Filter, join, and save by stmt
        FilterProcess(
            db_dir=configuration.db_dir,
            target_dir=filtered_quarterly_joined_by_stmt_dir,
            bag_type=&#34;joined&#34;,
            save_by_stmt=True,
            execute_serial=not filter_parallelize,
            file_type=&#34;quarter&#34;,
        )
    )

    processes.append(
        # 2. Standardize the data for every quarter
        StandardizeProcess(
            root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;, target_dir=standardized_quarterly_by_stmt_dir
        ),
    )

    processes.extend(
        [
            # 3. building datasets with all entries by stmt
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/CI&#34;,
                pathfilter=&#34;*/CI&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/CP&#34;,
                pathfilter=&#34;*/CP&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/EQ&#34;,
                pathfilter=&#34;*/EQ&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
            ),
        ]
    )

    # 4. create a single joined bag with all the data filtered and joined
    processes.append(
        ConcatByChangedTimestampProcess(
            root_dir=concat_quarterly_joined_by_stmt_dir,
            target_dir=concat_quarterly_joined_all_dir,
        )
    )

    # 5. concate the standardized bags together by stmt (BS, IS, CF).
    processes.extend(
        [
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_quarterly_by_stmt_dir,
                target_dir=f&#34;{concat_quarterly_standardized_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_quarterly_by_stmt_dir,
                target_dir=f&#34;{concat_quarterly_standardized_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_quarterly_by_stmt_dir,
                target_dir=f&#34;{concat_quarterly_standardized_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
        ]
    )

    # DAILY DATA Processing
    processes.append(LoggingProcess(title=&#34;Post Update Processes For Daily Data Started&#34;, lines=[]))

    # clean daily data covered now by quarterly data
    processes.append(
        ClearDailyDataProcess(
            db_dir=configuration.db_dir,
            filtered_daily_joined_by_stmt_dir=filtered_daily_joined_by_stmt_dir,
            standardized_daily_by_stmt_dir=standardized_daily_by_stmt_dir,
        )
    )

    # 1. Filter, join, and save by stmt
    processes.append(
        FilterProcess(
            db_dir=configuration.db_dir,
            target_dir=filtered_daily_joined_by_stmt_dir,
            bag_type=&#34;joined&#34;,
            save_by_stmt=True,
            execute_serial=not filter_parallelize,
            file_type=&#34;daily&#34;,
        )
    )

    processes.append(
        # 2. Standardize the data for daily data
        StandardizeProcess(
            root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;, target_dir=standardized_daily_by_stmt_dir
        ),
    )

    processes.extend(
        [
            # 3. building datasets with all entries by stmt for daily data
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/CI&#34;,
                pathfilter=&#34;*/CI&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/CP&#34;,
                pathfilter=&#34;*/CP&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/EQ&#34;,
                pathfilter=&#34;*/EQ&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
            ),
        ]
    )

    # 4. create a single joined bag with all the data filtered and joined for daily data
    processes.append(
        ConcatByChangedTimestampProcess(
            root_dir=concat_daily_joined_by_stmt_dir,
            target_dir=concat_daily_joined_all_dir,
        )
    )

    # 5. concate the standardized bags together by stmt (BS, IS, CF) for daily data.
    processes.extend(
        [
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_daily_by_stmt_dir,
                target_dir=f&#34;{concat_daily_standardized_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_daily_by_stmt_dir,
                target_dir=f&#34;{concat_daily_standardized_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_daily_by_stmt_dir,
                target_dir=f&#34;{concat_daily_standardized_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
        ]
    )

    # Concat daily and quarter together
    processes.append(
        LoggingProcess(title=&#34;Post Update Processes To Combine Quarterly And Daily Data Started&#34;, lines=[])
    )

    # 1. concat joined_by_statement
    processes.extend(
        [
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;BS&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;CF&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/CI&#34;,
                pathfilter=&#34;CI&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/CP&#34;,
                pathfilter=&#34;CP&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/EQ&#34;,
                pathfilter=&#34;EQ&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;IS&#34;,
            ),
        ]
    )

    # 2. concat joined
    processes.append(
        ConcatMultiRootByChangedTimestampProcess(
            root_dirs=[concat_daily_joined_all_dir, concat_quarterly_joined_all_dir],
            pathfilter=&#34;&#34;,
            target_dir=concat_all_joined_dir,
        )
    )

    # 3. concat standardized by statement
    processes.extend(
        [
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_daily_standardized_by_stmt_dir, concat_quarterly_standardized_by_stmt_dir],
                target_dir=f&#34;{concat_all_standardized_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;BS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_daily_standardized_by_stmt_dir, concat_quarterly_standardized_by_stmt_dir],
                target_dir=f&#34;{concat_all_standardized_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;CF&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_daily_standardized_by_stmt_dir, concat_quarterly_standardized_by_stmt_dir],
                target_dir=f&#34;{concat_all_standardized_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;IS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
        ]
    )
    return processes</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="secfsdstools.x_examples.automation.memory_optimized_daily_automation.define_extra_processes"><code class="name flex">
<span>def <span class="ident">define_extra_processes</span></span>(<span>configuration: <a title="secfsdstools.a_config.configmodel.Configuration" href="../../a_config/configmodel.html#secfsdstools.a_config.configmodel.Configuration">Configuration</a>) ‑> List[<a title="secfsdstools.c_automation.task_framework.AbstractProcess" href="../../c_automation/task_framework.html#secfsdstools.c_automation.task_framework.AbstractProcess">AbstractProcess</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>example definition of an additional pipeline. It adds sevreal steps to process
quarterly and daily data, as well as combining both. See the documentation of this
module for more details.</p>
<p>All these steps have a low memory footprint, so, the should run without any problems also
on 16 GB machine.</p>
<p>Please have a look at the notebook 08_00_automation_basics for further details.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>configuration</code></strong></dt>
<dd>the configuration</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[AbstractProcess]</code></dt>
<dd>List with the defined process steps</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def define_extra_processes(configuration: Configuration) -&gt; List[AbstractProcess]:
    &#34;&#34;&#34;
    example definition of an additional pipeline. It adds sevreal steps to process
    quarterly and daily data, as well as combining both. See the documentation of this
    module for more details.

    All these steps have a low memory footprint, so, the should run without any problems also
    on 16 GB machine.

    Please have a look at the notebook 08_00_automation_basics for further details.

    Args:
        configuration: the configuration

    Returns:
        List[AbstractProcess]: List with the defined process steps

    &#34;&#34;&#34;

    filtered_quarterly_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Filter&#34;, option=&#34;filtered_quarterly_joined_by_stmt_dir&#34;
    )
    filtered_daily_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Filter&#34;, option=&#34;filtered_daily_joined_by_stmt_dir&#34;
    )

    filter_parallelize = configuration.get_parser().get(section=&#34;Filter&#34;, option=&#34;parallelize&#34;, fallback=True)

    standardized_quarterly_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Standardizer&#34;, option=&#34;standardized_quarterly_by_stmt_dir&#34;
    )
    standardized_daily_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Standardizer&#34;, option=&#34;standardized_daily_by_stmt_dir&#34;
    )

    concat_quarterly_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_quarterly_joined_by_stmt_dir&#34;
    )
    concat_daily_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_daily_joined_by_stmt_dir&#34;
    )

    concat_quarterly_joined_all_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_quarterly_joined_all_dir&#34;
    )
    concat_daily_joined_all_dir = configuration.get_parser().get(section=&#34;Concat&#34;, option=&#34;concat_daily_joined_all_dir&#34;)

    concat_quarterly_standardized_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_quarterly_standardized_by_stmt_dir&#34;
    )
    concat_daily_standardized_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_daily_standardized_by_stmt_dir&#34;
    )

    concat_all_joined_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_all_joined_by_stmt_dir&#34;
    )

    concat_all_joined_dir = configuration.get_parser().get(section=&#34;Concat&#34;, option=&#34;concat_all_joined_dir&#34;)

    concat_all_standardized_by_stmt_dir = configuration.get_parser().get(
        section=&#34;Concat&#34;, option=&#34;concat_all_standardized_by_stmt_dir&#34;
    )

    processes: List[AbstractProcess] = []

    # QUARTERLY DATA Processing
    processes.append(LoggingProcess(title=&#34;Post Update Processes For Quarterly Data Started&#34;, lines=[]))

    processes.append(
        # 1. Filter, join, and save by stmt
        FilterProcess(
            db_dir=configuration.db_dir,
            target_dir=filtered_quarterly_joined_by_stmt_dir,
            bag_type=&#34;joined&#34;,
            save_by_stmt=True,
            execute_serial=not filter_parallelize,
            file_type=&#34;quarter&#34;,
        )
    )

    processes.append(
        # 2. Standardize the data for every quarter
        StandardizeProcess(
            root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;, target_dir=standardized_quarterly_by_stmt_dir
        ),
    )

    processes.extend(
        [
            # 3. building datasets with all entries by stmt
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/CI&#34;,
                pathfilter=&#34;*/CI&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/CP&#34;,
                pathfilter=&#34;*/CP&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/EQ&#34;,
                pathfilter=&#34;*/EQ&#34;,
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=f&#34;{filtered_quarterly_joined_by_stmt_dir}/quarter&#34;,
                target_dir=f&#34;{concat_quarterly_joined_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
            ),
        ]
    )

    # 4. create a single joined bag with all the data filtered and joined
    processes.append(
        ConcatByChangedTimestampProcess(
            root_dir=concat_quarterly_joined_by_stmt_dir,
            target_dir=concat_quarterly_joined_all_dir,
        )
    )

    # 5. concate the standardized bags together by stmt (BS, IS, CF).
    processes.extend(
        [
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_quarterly_by_stmt_dir,
                target_dir=f&#34;{concat_quarterly_standardized_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_quarterly_by_stmt_dir,
                target_dir=f&#34;{concat_quarterly_standardized_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_quarterly_by_stmt_dir,
                target_dir=f&#34;{concat_quarterly_standardized_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
        ]
    )

    # DAILY DATA Processing
    processes.append(LoggingProcess(title=&#34;Post Update Processes For Daily Data Started&#34;, lines=[]))

    # clean daily data covered now by quarterly data
    processes.append(
        ClearDailyDataProcess(
            db_dir=configuration.db_dir,
            filtered_daily_joined_by_stmt_dir=filtered_daily_joined_by_stmt_dir,
            standardized_daily_by_stmt_dir=standardized_daily_by_stmt_dir,
        )
    )

    # 1. Filter, join, and save by stmt
    processes.append(
        FilterProcess(
            db_dir=configuration.db_dir,
            target_dir=filtered_daily_joined_by_stmt_dir,
            bag_type=&#34;joined&#34;,
            save_by_stmt=True,
            execute_serial=not filter_parallelize,
            file_type=&#34;daily&#34;,
        )
    )

    processes.append(
        # 2. Standardize the data for daily data
        StandardizeProcess(
            root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;, target_dir=standardized_daily_by_stmt_dir
        ),
    )

    processes.extend(
        [
            # 3. building datasets with all entries by stmt for daily data
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/CI&#34;,
                pathfilter=&#34;*/CI&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/CP&#34;,
                pathfilter=&#34;*/CP&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/EQ&#34;,
                pathfilter=&#34;*/EQ&#34;,
            ),
            ConcatByChangedTimestampProcess(
                root_dir=f&#34;{filtered_daily_joined_by_stmt_dir}/daily&#34;,
                target_dir=f&#34;{concat_daily_joined_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
            ),
        ]
    )

    # 4. create a single joined bag with all the data filtered and joined for daily data
    processes.append(
        ConcatByChangedTimestampProcess(
            root_dir=concat_daily_joined_by_stmt_dir,
            target_dir=concat_daily_joined_all_dir,
        )
    )

    # 5. concate the standardized bags together by stmt (BS, IS, CF) for daily data.
    processes.extend(
        [
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_daily_by_stmt_dir,
                target_dir=f&#34;{concat_daily_standardized_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;*/BS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_daily_by_stmt_dir,
                target_dir=f&#34;{concat_daily_standardized_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;*/CF&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatByNewSubfoldersProcess(
                root_dir=standardized_daily_by_stmt_dir,
                target_dir=f&#34;{concat_daily_standardized_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;*/IS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
        ]
    )

    # Concat daily and quarter together
    processes.append(
        LoggingProcess(title=&#34;Post Update Processes To Combine Quarterly And Daily Data Started&#34;, lines=[])
    )

    # 1. concat joined_by_statement
    processes.extend(
        [
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;BS&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;CF&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/CI&#34;,
                pathfilter=&#34;CI&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/CP&#34;,
                pathfilter=&#34;CP&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/EQ&#34;,
                pathfilter=&#34;EQ&#34;,
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_quarterly_joined_by_stmt_dir, concat_daily_joined_by_stmt_dir],
                target_dir=f&#34;{concat_all_joined_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;IS&#34;,
            ),
        ]
    )

    # 2. concat joined
    processes.append(
        ConcatMultiRootByChangedTimestampProcess(
            root_dirs=[concat_daily_joined_all_dir, concat_quarterly_joined_all_dir],
            pathfilter=&#34;&#34;,
            target_dir=concat_all_joined_dir,
        )
    )

    # 3. concat standardized by statement
    processes.extend(
        [
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_daily_standardized_by_stmt_dir, concat_quarterly_standardized_by_stmt_dir],
                target_dir=f&#34;{concat_all_standardized_by_stmt_dir}/BS&#34;,
                pathfilter=&#34;BS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_daily_standardized_by_stmt_dir, concat_quarterly_standardized_by_stmt_dir],
                target_dir=f&#34;{concat_all_standardized_by_stmt_dir}/CF&#34;,
                pathfilter=&#34;CF&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
            ConcatMultiRootByChangedTimestampProcess(
                root_dirs=[concat_daily_standardized_by_stmt_dir, concat_quarterly_standardized_by_stmt_dir],
                target_dir=f&#34;{concat_all_standardized_by_stmt_dir}/IS&#34;,
                pathfilter=&#34;IS&#34;,
                in_memory=True,  # Standardized Bag only work with in_memory
            ),
        ]
    )
    return processes</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess"><code class="flex name class">
<span>class <span class="ident">ClearDailyDataProcess</span></span>
<span>(</span><span>db_dir: str, filtered_daily_joined_by_stmt_dir: str, standardized_daily_by_stmt_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Since daily processed data is removed, as soon as the data is contained in a quarterly zip file,
we also need to remove the according data from the automated datasets.</p>
<p>This process does this for the filtered and standardized daily data, which are created in the first step
of processing the daily data.</p>
<p>The following steps for the daily processing always creates the dataset fresh from all available daily data.
Hence, we only need to clean the filtered and standardized daily data.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>db_dir</code></strong></dt>
<dd>directory of the database</dd>
<dt><strong><code>filtered_daily_joined_by_stmt_dir</code></strong></dt>
<dd>directory containing the filtered daily data</dd>
<dt><strong><code>standardized_daily_by_stmt_dir</code></strong></dt>
<dd>directory containing the standardized daily data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClearDailyDataProcess(AbstractProcess):
    &#34;&#34;&#34;
    Since daily processed data is removed, as soon as the data is contained in a quarterly zip file,
    we also need to remove the according data from the automated datasets.

    This process does this for the filtered and standardized daily data, which are created in the first step
    of processing the daily data.

    The following steps for the daily processing always creates the dataset fresh from all available daily data.
    Hence, we only need to clean the filtered and standardized daily data.
    &#34;&#34;&#34;

    def __init__(self, db_dir: str, filtered_daily_joined_by_stmt_dir: str, standardized_daily_by_stmt_dir: str):
        &#34;&#34;&#34;
        Constructor.
        Args:
            db_dir: directory of the database
            filtered_daily_joined_by_stmt_dir: directory containing the filtered daily data
            standardized_daily_by_stmt_dir: directory containing the standardized daily data
        &#34;&#34;&#34;
        super().__init__()
        self.db_dir = db_dir
        self.index_accessor = ParquetDBIndexingAccessor(db_dir=db_dir)
        self.filtered_daily_joined_by_stmt_dir = filtered_daily_joined_by_stmt_dir
        self.standardized_daily_by_stmt_dir = standardized_daily_by_stmt_dir

    def clear_directory(self, cut_off_day: int, root_dir_path: Path):
        &#34;&#34;&#34;
        removes all directories under root_dir_path that are older than the cut_off_day.
        &#34;&#34;&#34;

        cut_off_file_name = f&#34;{cut_off_day}.zip&#34;

        if root_dir_path.exists():
            for dir_path in root_dir_path.iterdir():
                if dir_path.is_dir() and dir_path.name &lt; cut_off_file_name:
                    shutil.rmtree(dir_path)

    def process(self):
        &#34;&#34;&#34;
        Execute the process.
        &#34;&#34;&#34;
        last_processed_quarter_file_name = self.index_accessor.find_latest_quarter_file_name()
        if last_processed_quarter_file_name is None:
            raise ValueError(
                &#34;No quarterly files were processed before. &#34;
                &#34;Please process quarterly files first before running the daily process.&#34;
            )
        last_processed_quarter = last_processed_quarter_file_name.split(&#34;.&#34;)[0]

        daily_start_quarter = DailyPreparationProcess.calculate_daily_start_quarter(last_processed_quarter)
        cut_off_day = DailyPreparationProcess.cut_off_day(daily_start_quarter)
        self.clear_directory(
            cut_off_day=cut_off_day, root_dir_path=Path(self.filtered_daily_joined_by_stmt_dir) / &#34;daily&#34;
        )
        self.clear_directory(cut_off_day=cut_off_day, root_dir_path=Path(self.standardized_daily_by_stmt_dir))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="secfsdstools.c_automation.task_framework.AbstractProcess" href="../../c_automation/task_framework.html#secfsdstools.c_automation.task_framework.AbstractProcess">AbstractProcess</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess.clear_directory"><code class="name flex">
<span>def <span class="ident">clear_directory</span></span>(<span>self, cut_off_day: int, root_dir_path: pathlib.Path)</span>
</code></dt>
<dd>
<div class="desc"><p>removes all directories under root_dir_path that are older than the cut_off_day.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear_directory(self, cut_off_day: int, root_dir_path: Path):
    &#34;&#34;&#34;
    removes all directories under root_dir_path that are older than the cut_off_day.
    &#34;&#34;&#34;

    cut_off_file_name = f&#34;{cut_off_day}.zip&#34;

    if root_dir_path.exists():
        for dir_path in root_dir_path.iterdir():
            if dir_path.is_dir() and dir_path.name &lt; cut_off_file_name:
                shutil.rmtree(dir_path)</code></pre>
</details>
</dd>
<dt id="secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Execute the process.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(self):
    &#34;&#34;&#34;
    Execute the process.
    &#34;&#34;&#34;
    last_processed_quarter_file_name = self.index_accessor.find_latest_quarter_file_name()
    if last_processed_quarter_file_name is None:
        raise ValueError(
            &#34;No quarterly files were processed before. &#34;
            &#34;Please process quarterly files first before running the daily process.&#34;
        )
    last_processed_quarter = last_processed_quarter_file_name.split(&#34;.&#34;)[0]

    daily_start_quarter = DailyPreparationProcess.calculate_daily_start_quarter(last_processed_quarter)
    cut_off_day = DailyPreparationProcess.cut_off_day(daily_start_quarter)
    self.clear_directory(
        cut_off_day=cut_off_day, root_dir_path=Path(self.filtered_daily_joined_by_stmt_dir) / &#34;daily&#34;
    )
    self.clear_directory(cut_off_day=cut_off_day, root_dir_path=Path(self.standardized_daily_by_stmt_dir))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="secfsdstools.x_examples.automation" href="index.html">secfsdstools.x_examples.automation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="secfsdstools.x_examples.automation.memory_optimized_daily_automation.define_extra_processes" href="#secfsdstools.x_examples.automation.memory_optimized_daily_automation.define_extra_processes">define_extra_processes</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess" href="#secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess">ClearDailyDataProcess</a></code></h4>
<ul class="">
<li><code><a title="secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess.clear_directory" href="#secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess.clear_directory">clear_directory</a></code></li>
<li><code><a title="secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess.process" href="#secfsdstools.x_examples.automation.memory_optimized_daily_automation.ClearDailyDataProcess.process">process</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>