{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9421597-348f-463b-8438-6ea613e4b265",
   "metadata": {},
   "source": [
    "# Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd92aaa-a8df-4c3a-8062-f934759ed786",
   "metadata": {},
   "source": [
    "## TLDR\n",
    "\n",
    "- The automation features gives you the possibility to add additional processing steps after the default update steps: download of new zip files from SEC, transforming them to Parquet format and indexing them in the SQLite DB. This means, when a new zip file is detected at SEC, not only the mentioned three steps are automatcially executed but also additional steps that you define by yourself.\n",
    "- There are two hook methods you can define and use to implement additional logic. Both of them are activated by defining them in the configuration file.\n",
    "- The simpler of this two hook methods just receives the `Configuration` object and is called after the all updated steps, the default update stapes (downloading of zip files, transform to parquet, indexing) and additional user defined update steps, were executed.\n",
    "- The more complex one receives a `Configuration` object and has to return a list of instances derived from `AbstractProcess`. There are some basic implementations of `AbstractProcess`, that can be used to filter, concat, and standardize the data.\n",
    "- The library contains an example implementation of a hook function which filters the data and also directly applies the standardizer for balance sheet, income statement, and cash flow. See below on how it is implemented and how it can be directly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7324a-2a5a-47b4-92d6-9f18342863b9",
   "metadata": {},
   "source": [
    "## Defining a simple postupdatehook function\n",
    "\n",
    "If you define a postupdatehook function in the configuration file then this function will be called after the all update steps were executed.\n",
    "\n",
    "It will be called, regardless if the previous steps did actually do something. For instance, if there was now new zip file detected to download, it will be called anyway, but not more than once every 24 hours (the usual period the framework checks for upates).\n",
    "\n",
    "Since the hook method is called even if there were no updates, it is your responsibility to check if actually something did change. Otherwise, if you implemented time consuming logic, it would be executed every 24 hours once.\n",
    "\n",
    "The postupdatehook function needs a `Configuration` parameter and does not return anything. It can have any name you like.\n",
    "\n",
    "<pre>\n",
    "# it is ok to import the Configuration class\n",
    "from secfsdstools.a_config.configmodel import Configuration \n",
    "from secfsdstools.c_index.indexdataaccess import ParquetDBIndexingAccessor\n",
    "from secfsdstools... import ...\n",
    "\n",
    "def my_postupdatehook_function(configuration: Configuration):\n",
    "    \n",
    "    # you can use the configuration for instance to instantiate access to the SQLite db\n",
    "    index_db = ParquetDBIndexingAccessor(db_dir=configuration.db_dir)\n",
    "    ...\n",
    "    \n",
    "</pre>\n",
    "\n",
    "To activate it, just define it in the DEFAULT section of the configuration file:\n",
    "\n",
    "<pre>\n",
    "[DEFAULT]\n",
    "downloaddirectory = C:/data/sec/automated/dld\n",
    "dbdirectory = C:/data/sec/automated/db\n",
    "parquetdirectory = C:/data/sec/automated/parquet\n",
    "useragentemail = your.email@goeshere.com\n",
    "autoupdate = True\n",
    "keepzipfiles = False\n",
    "postupdatehook = mypackage.mymodule.my_postupdatehook_function\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe8976-ad39-48d8-96a3-5efd6a13c4a7",
   "metadata": {},
   "source": [
    "## Defining a postupdateprocesses function\n",
    "\n",
    "If you define a postupdateprocess function, it has to return a list of instances of `AbtractProcess`. These instances are then executed after the default steps download, transform to parquet, and indexing were executed.\n",
    "\n",
    "Also here, every \"process\" will be called once every 24 hours, and therefore, every process implementation has to check itself if something changed.\n",
    "\n",
    "As a parameter, the postupdatedprocesses function must have a `Configuration` parameter and also has to return list of instances `AbstractProcess`.\n",
    "\n",
    "*Note: There are some basic implementations of the `AbstractProcess` class within the `secfsdstools.g_pipelines` package that provide implementation to filter, to concat bags, and to standardize joined bags. \n",
    "Please have a look at the following section which show an example on how this basic implementations can be used.*\n",
    "\n",
    "<pre>\n",
    "# it is ok to import the Configuration and AbstractProcess classes\n",
    "from secfsdstools.a_config.configmodel import Configuration \n",
    "from secfsdstools.c_automation.task_framework import AbstractProcess\n",
    "\n",
    "\n",
    "def my_postupdateprocesses_function(configuration: Configuration) -> List[AbstractProcess]:\n",
    "    # do your secfsdstools imprts here\n",
    "    from secfsdstools... import ...\n",
    "    \n",
    "    processes: List[AbstractProcess] = []\n",
    "    ...\n",
    "    \n",
    "    return processes\n",
    "    \n",
    "</pre>\n",
    "\n",
    "To activate it, added the appropriate configuration in the DEFAULT section of the configuration file:\n",
    "\n",
    "<pre>\n",
    "[DEFAULT]\n",
    "downloaddirectory = C:/data/sec/automated/dld\n",
    "dbdirectory = C:/data/sec/automated/db\n",
    "parquetdirectory = C:/data/sec/automated/parquet\n",
    "useragentemail = your.email@goeshere.com\n",
    "autoupdate = True\n",
    "keepzipfiles = False\n",
    "postupdateprocesses = mypackage.mymodule.my_postupdateprocesses_function\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d885a-5b13-4803-8443-de1437f2f01b",
   "metadata": {},
   "source": [
    "## A working example of the postupdateprocesses function\n",
    "### How to use the example\n",
    "\n",
    "The package `secfsdstools.x_examples.automation provides` a default implemention of a postupdateprocesses function: `define_extra_processes`.\n",
    "\n",
    "You can use this function directly by adding it to your configuration file together with some additional configuration parameters used by it: \n",
    "<pre>\n",
    "[DEFAULT]\n",
    "...\n",
    "postupdateprocesses=secfsdstools.x_examples.automation.automation.define_extra_processes\n",
    "\n",
    "[Filter]\n",
    "filtered_dir_by_stmt_joined = C:/data/sec/automated/_1_filtered_by_stmt_joined\n",
    "\n",
    "[Concat]\n",
    "concat_dir_by_stmt_joined = C:/data/sec/automated/_2_concat_by_stmt_joined\n",
    "\n",
    "[Standardizer]\n",
    "standardized_dir = C:/data/sec/automated/_3_standardized\n",
    "\n",
    "; [SingleBag]\n",
    "; singlebag_dir = C:/data/sec/automated/_4_single_bag\n",
    "</pre>\n",
    "\n",
    "The function will add 3 additional steps and a fourth optional step. The optional step is only executed if the needed parameter `singlebag_dir` is defined. \n",
    "\n",
    "These steps add the following processing:\n",
    "\n",
    "The first step creates a joined bag for every zip file which is filtered for 10-K and 10-Q reports only\n",
    "and also applies the filters `ReportPeriodRawFilter`, `MainCoregRawFilter`, `USDOnlyRawFilter`, `OfficialTagsOnlyRawFilter`. \n",
    "Furthermore, the data is also split by stmt.\n",
    "The filtered joined bag is stored under the path that is defined under `filtered_dir_by_stmt_joined` in the configuration file.\n",
    "The resulting directory structure will look like this:\n",
    "\n",
    "\n",
    "    <filtered_dir_by_stmt_joined>\n",
    "        quarter\n",
    "            2009q2.zip\n",
    "                BS\n",
    "                CF\n",
    "                CI\n",
    "                CP\n",
    "                EQ\n",
    "                IS\n",
    "            ...\n",
    "\n",
    "The second step creates a single joined bag for every statement (balance sheet, income statement,\n",
    "cash flow, cover page, ...) that contains the data from all zip files, resp from all the\n",
    "available quarters. These bags are stored under the path defined as `concat_dir_by_stmt_joined`.\n",
    "The resulting directory structure will look like this:\n",
    "\n",
    "    <concat_dir_by_stmt_joined>\n",
    "        BS\n",
    "        CF\n",
    "        CI\n",
    "        CP\n",
    "        EQ\n",
    "        IS    \n",
    "\n",
    "\n",
    "The third step standardizes the data for balance sheet, income statement, and cash flow and stores\n",
    "the standardized bags under the path that is defined as `standardized_dir`.\n",
    "The resulting structure will look like this:\n",
    "\n",
    "    <standardized_dir>\n",
    "        BS\n",
    "        CF\n",
    "        IS    \n",
    "    \n",
    "\n",
    "The fourth step is optional and is only executed if the configuration file contains an entry\n",
    "for `singlebag_dir`. If it does, it will create a single joined bag concatenating all the bags\n",
    "created in the second step, so basically creating a single bag that contains all the filtered data from\n",
    "all the available zip files, resp. quarters. This step needs more memory than the others, so it\n",
    "might not be running on every system.\n",
    "The resulting directory structure will look like this:\n",
    "\n",
    "    <singlebag_dir>\n",
    "        all\n",
    "\n",
    "\n",
    "Hint -> data can directly be loaded with the JoinedDataBag load, resp with STandardizedBag load.\n",
    "\n",
    "\n",
    "### How the example is implemented.\n",
    "\n",
    "Let us have a look at the implementation of the the function `define_extra_processes`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647f56f-be58-4323-b7b4-16a369b2cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from secfsdstools.a_config.configmodel import Configuration\n",
    "from secfsdstools.c_automation.task_framework import AbstractProcess\n",
    "\n",
    "# Note: that we are using a few basic implementations of the g_pipeline package\n",
    "from secfsdstools.g_pipelines.concat_process import ConcatByNewSubfoldersProcess, \\\n",
    "    ConcatByChangedTimestampProcess\n",
    "from secfsdstools.g_pipelines.filter_process import FilterProcess\n",
    "from secfsdstools.g_pipelines.standardize_process import StandardizeProcess\n",
    "\n",
    "\n",
    "def define_extra_processes(configuration: Configuration) -> List[AbstractProcess]:\n",
    "\n",
    "    # first, we read the configuration parameters. \n",
    "    joined_by_stmt_dir = configuration.config_parser.get(section=\"Filter\",\n",
    "                                                         option=\"filtered_dir_by_stmt_joined\")\n",
    "\n",
    "    concat_by_stmt_dir = configuration.config_parser.get(section=\"Concat\",\n",
    "                                                         option=\"concat_dir_by_stmt_joined\")\n",
    "\n",
    "    standardized_dir = configuration.config_parser.get(section=\"Standardizer\",\n",
    "                                                       option=\"standardized_dir\")\n",
    "\n",
    "    # note that the single bag dir is optional and therefore has a fallback value of \"\"\n",
    "    singlebag_dir = configuration.config_parser.get(section=\"SingleBag\",\n",
    "                                                    option=\"singlebag_dir\",\n",
    "                                                    fallback=\"\")\n",
    "\n",
    "    \n",
    "    processes: List[AbstractProcess] = []\n",
    "\n",
    "    # The first step filters the data. It is apllied on the data of every available transformed parquet folder.\n",
    "    # If nothing else is configured, it will filter for 10-K and 10-Q reports only.\n",
    "    # Moreover, it will also apply the filters ReportPeriodRawFilter, MainCoregRawFilter, USDOnlyRawFilter, and OfficialTagsOnlyRawFilter. \n",
    "    # You can actually configure wether you want the data to be saved as RawDataBag or a JoinedDataBag.\n",
    "    # In our case, we will use the JoinedDataBag.\n",
    "    # As another parameter, we can configure that the data is split up by stmt. So the data for every statement is saved in its on subfolder.\n",
    "    # Therefore, the result will be a folder for every quarter containing subfolders for every statement (BS, CF, CI, CP, EQ, and IS).\n",
    "    # Note that the execution processed in parallel.\n",
    "    processes.append(\n",
    "        # 1. Filter, join, and save by stmt\n",
    "        FilterProcess(db_dir=configuration.db_dir,\n",
    "                      target_dir=joined_by_stmt_dir,\n",
    "                      bag_type=\"joined\",\n",
    "                      save_by_stmt=True,\n",
    "                      execute_serial=configuration.no_parallel_processing\n",
    "                      )\n",
    "    )\n",
    "\n",
    "    # Next, we want to create a single JoinedBag for every stmt that should contain the data from all quarters.\n",
    "    # To do that, we can use the ConcatByNewSubfoldersProcess. \n",
    "    # It will load the joined bags from the appropriate subfolders inside the root_dir, concat them into single \n",
    "    # bag that is then stored in the target_dir.\n",
    "    # Note: This AbstractProcess implementation stores a metainf file in the target folder which contains a list\n",
    "    # of all the subfolders from the root-dir, that are already concatenated into the target. Using that file,\n",
    "    # it will only be executed, if a new subfolder appears in the root_dir.\n",
    "    # After these steps, we will have a joined bag for every statement (BS, CF, IS, ...) containing the data from all\n",
    "    # available quarters.\n",
    "    processes.extend([\n",
    "        # 2. building datasets with all entries by stmt\n",
    "        ConcatByNewSubfoldersProcess(root_dir=f\"{joined_by_stmt_dir}/quarter\",\n",
    "                                     target_dir=f\"{concat_by_stmt_dir}/BS\",\n",
    "                                     pathfilter=\"*/BS\"\n",
    "                                     ),\n",
    "        ConcatByNewSubfoldersProcess(root_dir=f\"{joined_by_stmt_dir}/quarter\",\n",
    "                                     target_dir=f\"{concat_by_stmt_dir}/CF\",\n",
    "                                     pathfilter=\"*/CF\"\n",
    "                                     ),\n",
    "        ConcatByNewSubfoldersProcess(root_dir=f\"{joined_by_stmt_dir}/quarter\",\n",
    "                                     target_dir=f\"{concat_by_stmt_dir}/CI\",\n",
    "                                     pathfilter=\"*/CI\"\n",
    "                                     ),\n",
    "        ConcatByNewSubfoldersProcess(root_dir=f\"{joined_by_stmt_dir}/quarter\",\n",
    "                                     target_dir=f\"{concat_by_stmt_dir}/CP\",\n",
    "                                     pathfilter=\"*/CP\"\n",
    "                                     ),\n",
    "        ConcatByNewSubfoldersProcess(root_dir=f\"{joined_by_stmt_dir}/quarter\",\n",
    "                                     target_dir=f\"{concat_by_stmt_dir}/EQ\",\n",
    "                                     pathfilter=\"*/EQ\"\n",
    "                                     ),\n",
    "        ConcatByNewSubfoldersProcess(root_dir=f\"{joined_by_stmt_dir}/quarter\",\n",
    "                                     target_dir=f\"{concat_by_stmt_dir}/IS\",\n",
    "                                     pathfilter=\"*/IS\"\n",
    "                                     )\n",
    "    ])\n",
    "\n",
    "    # As a third step, we standardize the data for BS, CF, and IS. \n",
    "    # This can be done with the StandardizeProcess. As input it expects a folder that contains the subfolders BS, CF, and IS\n",
    "    processes.append(\n",
    "        # 3. Standardize the data\n",
    "        StandardizeProcess(root_dir=f\"{concat_by_stmt_dir}\",\n",
    "                           target_dir=standardized_dir),\n",
    "    )\n",
    "\n",
    "    # If the parameter singlebag_dir is configured, we use the ConcatByChangedTimestampProcess\n",
    "    # to concat all the bags from the second step together into a single bag. \n",
    "    # This produces a single bag containing all the available data.\n",
    "    # Note: this uses a lot of memory\n",
    "    # Note: this implementation stores a metainf file in the target folder, that contains the \n",
    "    # timestamp of the latest modifications inside the root_dir. Threfore, it will only be executed\n",
    "    # if any file in the root_dir has a newer timestamp than the saved one.\n",
    "    if singlebag_dir != \"\":\n",
    "        # 4. create a single joined bag with all the data, if it is defined\n",
    "        processes.append(\n",
    "            ConcatByChangedTimestampProcess(\n",
    "                root_dir=f\"{concat_by_stmt_dir}/\",\n",
    "                target_dir=f\"{singlebag_dir}/all\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5a16b-7cce-4e9c-b2cc-818b2c7a8ea7",
   "metadata": {},
   "source": [
    "If you want to know more about the implementation details of the FilterProcess, ConcatByNewSubfoldersProcess, ConcatByChangedTimestampProcess, and StandardizeProcess, have a look at the comments in the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
